{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Daniele\\\\PycharmProjects\\\\LipNetProve\\\\Dataset\\\\Train'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../Dataset/Train')\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data=[]\n",
    "for l in os.listdir():\n",
    "    os.chdir(l)\n",
    "    for m in os.listdir():\n",
    "        data.append([l+'/'+m, l])\n",
    "    os.chdir('..')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[['a/F04_phrases01_02.jpg', 'a'],\n ['a/F04_phrases01_03.jpg', 'a'],\n ['a/F04_phrases01_04.jpg', 'a'],\n ['a/F04_phrases01_05.jpg', 'a'],\n ['a/F04_phrases01_06.jpg', 'a'],\n ['a/F04_phrases01_07.jpg', 'a'],\n ['a/F05_phrases01_01.jpg', 'a'],\n ['a/F05_phrases01_02.jpg', 'a'],\n ['a/F05_phrases01_03.jpg', 'a'],\n ['a/F05_phrases01_04.jpg', 'a'],\n ['a/F05_phrases01_05.jpg', 'a'],\n ['a/F05_phrases01_06.jpg', 'a'],\n ['a/F05_phrases01_07.jpg', 'a'],\n ['a/F05_phrases01_08.jpg', 'a'],\n ['a/F05_phrases01_09.jpg', 'a'],\n ['a/F05_phrases01_10.jpg', 'a'],\n ['a/F06_phrases01_01.jpg', 'a'],\n ['a/F06_phrases01_02.jpg', 'a'],\n ['a/F06_phrases01_03.jpg', 'a'],\n ['a/F06_phrases01_04.jpg', 'a'],\n ['a/F06_phrases01_05.jpg', 'a'],\n ['a/F06_phrases01_07.jpg', 'a'],\n ['a/F06_phrases01_08.jpg', 'a'],\n ['a/F06_phrases01_09.jpg', 'a'],\n ['a/F06_phrases01_10.jpg', 'a'],\n ['a/F07_phrases01_01.jpg', 'a'],\n ['a/F07_phrases01_02.jpg', 'a'],\n ['a/F07_phrases01_03.jpg', 'a'],\n ['a/F07_phrases01_04.jpg', 'a'],\n ['a/F07_phrases01_05.jpg', 'a'],\n ['a/F08_phrases01_04.jpg', 'a'],\n ['a/F08_phrases01_05.jpg', 'a'],\n ['a/F08_phrases01_06.jpg', 'a'],\n ['a/F08_phrases01_07.jpg', 'a'],\n ['a/F08_phrases01_08.jpg', 'a'],\n ['a/F08_phrases01_09.jpg', 'a'],\n ['a/F08_phrases01_10.jpg', 'a'],\n ['a/F09_phrases01_01.jpg', 'a'],\n ['a/F09_phrases01_02.jpg', 'a'],\n ['a/F09_phrases01_03.jpg', 'a'],\n ['a/F09_phrases01_04.jpg', 'a'],\n ['a/F09_phrases01_05.jpg', 'a'],\n ['a/F09_phrases01_06.jpg', 'a'],\n ['a/F09_phrases01_09.jpg', 'a'],\n ['a/F09_phrases01_10.jpg', 'a'],\n ['a/F10_phrases01_01.jpg', 'a'],\n ['a/F10_phrases01_02.jpg', 'a'],\n ['a/F10_phrases01_03.jpg', 'a'],\n ['a/F10_phrases01_04.jpg', 'a'],\n ['a/F10_phrases01_05.jpg', 'a'],\n ['a/F10_phrases01_06.jpg', 'a'],\n ['a/F10_phrases01_07.jpg', 'a'],\n ['a/F10_phrases01_08.jpg', 'a'],\n ['a/F10_phrases01_09.jpg', 'a'],\n ['a/F10_phrases01_10.jpg', 'a'],\n ['a/F11_phrases01_01.jpg', 'a'],\n ['a/F11_phrases01_02.jpg', 'a'],\n ['a/F11_phrases01_03.jpg', 'a'],\n ['a/F11_phrases01_04.jpg', 'a'],\n ['a/F11_phrases01_05.jpg', 'a'],\n ['a/F11_phrases01_06.jpg', 'a'],\n ['a/F11_phrases01_07.jpg', 'a'],\n ['a/F11_phrases01_08.jpg', 'a'],\n ['a/F11_phrases01_09.jpg', 'a'],\n ['a/F11_phrases01_10.jpg', 'a'],\n ['a/M01_phrases01_01.jpg', 'a'],\n ['a/M01_phrases01_02.jpg', 'a'],\n ['a/M01_phrases01_03.jpg', 'a'],\n ['a/M01_phrases01_04.jpg', 'a'],\n ['a/M01_phrases01_05.jpg', 'a'],\n ['a/M01_phrases01_06.jpg', 'a'],\n ['a/M01_phrases01_07.jpg', 'a'],\n ['a/M01_phrases01_08.jpg', 'a'],\n ['a/M01_phrases01_09.jpg', 'a'],\n ['a/M01_phrases01_10.jpg', 'a'],\n ['a/M02_phrases01_01.jpg', 'a'],\n ['a/M02_phrases01_02.jpg', 'a'],\n ['a/M02_phrases01_03.jpg', 'a'],\n ['a/M02_phrases01_04.jpg', 'a'],\n ['a/M02_phrases01_05.jpg', 'a'],\n ['a/M02_phrases01_06.jpg', 'a'],\n ['a/M02_phrases01_07.jpg', 'a'],\n ['a/M02_phrases01_08.jpg', 'a'],\n ['a/M02_phrases01_09.jpg', 'a'],\n ['a/M02_phrases01_10.jpg', 'a'],\n ['a/M04_phrases01_01.jpg', 'a'],\n ['a/M04_phrases01_02.jpg', 'a'],\n ['a/M04_phrases01_03.jpg', 'a'],\n ['a/M04_phrases01_04.jpg', 'a'],\n ['a/M04_phrases01_05.jpg', 'a'],\n ['a/M04_phrases01_06.jpg', 'a'],\n ['a/M04_phrases01_07.jpg', 'a'],\n ['a/M04_phrases01_08.jpg', 'a'],\n ['a/M04_phrases01_09.jpg', 'a'],\n ['a/M04_phrases01_10.jpg', 'a'],\n ['a/M07_phrases01_01.jpg', 'a'],\n ['a/M07_phrases01_02.jpg', 'a'],\n ['a/M07_phrases01_03.jpg', 'a'],\n ['a/M07_phrases01_04.jpg', 'a'],\n ['a/M07_phrases01_05.jpg', 'a'],\n ['a/M07_phrases01_06.jpg', 'a'],\n ['a/M07_phrases01_07.jpg', 'a'],\n ['a/M07_phrases01_08.jpg', 'a'],\n ['a/M07_phrases01_09.jpg', 'a'],\n ['a/M07_phrases01_10.jpg', 'a'],\n ['a/M08_phrases01_01.jpg', 'a'],\n ['a/M08_phrases01_02.jpg', 'a'],\n ['a/M08_phrases01_03.jpg', 'a'],\n ['a/M08_phrases01_04.jpg', 'a'],\n ['a/M08_phrases01_05.jpg', 'a'],\n ['a/M08_phrases01_06.jpg', 'a'],\n ['a/M08_phrases01_07.jpg', 'a'],\n ['a/M08_phrases01_08.jpg', 'a'],\n ['a/M08_phrases01_09.jpg', 'a'],\n ['a/M08_phrases01_10.jpg', 'a'],\n ['b/F01_phrases02_01.jpg', 'b'],\n ['b/F01_phrases02_02.jpg', 'b'],\n ['b/F01_phrases02_03.jpg', 'b'],\n ['b/F01_phrases02_04.jpg', 'b'],\n ['b/F01_phrases02_05.jpg', 'b'],\n ['b/F01_phrases02_06.jpg', 'b'],\n ['b/F01_phrases02_07.jpg', 'b'],\n ['b/F02_phrases02_05.jpg', 'b'],\n ['b/F02_phrases02_06.jpg', 'b'],\n ['b/F02_phrases02_07.jpg', 'b'],\n ['b/F02_phrases02_08.jpg', 'b'],\n ['b/F02_phrases02_09.jpg', 'b'],\n ['b/F02_phrases02_10.jpg', 'b'],\n ['b/F04_phrases02_01.jpg', 'b'],\n ['b/F04_phrases02_02.jpg', 'b'],\n ['b/F04_phrases02_03.jpg', 'b'],\n ['b/F04_phrases02_04.jpg', 'b'],\n ['b/F04_phrases02_05.jpg', 'b'],\n ['b/F04_phrases02_06.jpg', 'b'],\n ['b/F05_phrases02_02.jpg', 'b'],\n ['b/F05_phrases02_06.jpg', 'b'],\n ['b/F05_phrases02_07.jpg', 'b'],\n ['b/F05_phrases02_08.jpg', 'b'],\n ['b/F06_phrases02_02.jpg', 'b'],\n ['b/F06_phrases02_03.jpg', 'b'],\n ['b/F06_phrases02_04.jpg', 'b'],\n ['b/F06_phrases02_05.jpg', 'b'],\n ['b/F06_phrases02_06.jpg', 'b'],\n ['b/F06_phrases02_09.jpg', 'b'],\n ['b/F06_phrases02_10.jpg', 'b'],\n ['b/F07_phrases02_01.jpg', 'b'],\n ['b/F07_phrases02_02.jpg', 'b'],\n ['b/F07_phrases02_03.jpg', 'b'],\n ['b/F07_phrases02_04.jpg', 'b'],\n ['b/F07_phrases02_05.jpg', 'b'],\n ['b/F07_phrases02_06.jpg', 'b'],\n ['b/F07_phrases02_07.jpg', 'b'],\n ['b/F07_phrases02_08.jpg', 'b'],\n ['b/F07_phrases02_09.jpg', 'b'],\n ['b/F07_phrases02_10.jpg', 'b'],\n ['b/F08_phrases02_01.jpg', 'b'],\n ['b/F08_phrases02_02.jpg', 'b'],\n ['b/F08_phrases02_03.jpg', 'b'],\n ['b/F08_phrases02_04.jpg', 'b'],\n ['b/F08_phrases02_05.jpg', 'b'],\n ['b/F08_phrases02_06.jpg', 'b'],\n ['b/F08_phrases02_07.jpg', 'b'],\n ['b/F08_phrases02_08.jpg', 'b'],\n ['b/F08_phrases02_09.jpg', 'b'],\n ['b/F08_phrases02_10.jpg', 'b'],\n ['b/F09_phrases02_01.jpg', 'b'],\n ['b/F09_phrases02_02.jpg', 'b'],\n ['b/F09_phrases02_03.jpg', 'b'],\n ['b/F09_phrases02_04.jpg', 'b'],\n ['b/F09_phrases02_05.jpg', 'b'],\n ['b/F09_phrases02_06.jpg', 'b'],\n ['b/F09_phrases02_07.jpg', 'b'],\n ['b/F09_phrases02_08.jpg', 'b'],\n ['b/F09_phrases02_09.jpg', 'b'],\n ['b/F09_phrases02_10.jpg', 'b'],\n ['b/F10_phrases02_01.jpg', 'b'],\n ['b/F10_phrases02_02.jpg', 'b'],\n ['b/F10_phrases02_03.jpg', 'b'],\n ['b/F10_phrases02_04.jpg', 'b'],\n ['b/F10_phrases02_05.jpg', 'b'],\n ['b/F10_phrases02_06.jpg', 'b'],\n ['b/F10_phrases02_07.jpg', 'b'],\n ['b/F10_phrases02_08.jpg', 'b'],\n ['b/F10_phrases02_09.jpg', 'b'],\n ['b/F10_phrases02_10.jpg', 'b'],\n ['b/F11_phrases02_01.jpg', 'b'],\n ['b/F11_phrases02_02.jpg', 'b'],\n ['b/F11_phrases02_03.jpg', 'b'],\n ['b/F11_phrases02_04.jpg', 'b'],\n ['b/F11_phrases02_05.jpg', 'b'],\n ['b/F11_phrases02_06.jpg', 'b'],\n ['b/F11_phrases02_07.jpg', 'b'],\n ['b/F11_phrases02_08.jpg', 'b'],\n ['b/F11_phrases02_09.jpg', 'b'],\n ['b/F11_phrases02_10.jpg', 'b'],\n ['b/M01_phrases02_01.jpg', 'b'],\n ['b/M01_phrases02_02.jpg', 'b'],\n ['b/M01_phrases02_03.jpg', 'b'],\n ['b/M01_phrases02_04.jpg', 'b'],\n ['b/M01_phrases02_05.jpg', 'b'],\n ['b/M01_phrases02_06.jpg', 'b'],\n ['b/M01_phrases02_07.jpg', 'b'],\n ['b/M01_phrases02_08.jpg', 'b'],\n ['b/M01_phrases02_09.jpg', 'b'],\n ['b/M01_phrases02_10.jpg', 'b'],\n ['b/M02_phrases02_01.jpg', 'b'],\n ['b/M02_phrases02_02.jpg', 'b'],\n ['b/M02_phrases02_03.jpg', 'b'],\n ['b/M02_phrases02_04.jpg', 'b'],\n ['b/M02_phrases02_05.jpg', 'b'],\n ['b/M02_phrases02_06.jpg', 'b'],\n ['b/M02_phrases02_07.jpg', 'b'],\n ['b/M02_phrases02_08.jpg', 'b'],\n ['b/M02_phrases02_09.jpg', 'b'],\n ['b/M02_phrases02_10.jpg', 'b'],\n ['b/M04_phrases02_01.jpg', 'b'],\n ['b/M04_phrases02_02.jpg', 'b'],\n ['b/M04_phrases02_03.jpg', 'b'],\n ['b/M04_phrases02_04.jpg', 'b'],\n ['b/M04_phrases02_05.jpg', 'b'],\n ['b/M04_phrases02_06.jpg', 'b'],\n ['b/M04_phrases02_07.jpg', 'b'],\n ['b/M04_phrases02_08.jpg', 'b'],\n ['b/M04_phrases02_09.jpg', 'b'],\n ['b/M04_phrases02_10.jpg', 'b'],\n ['b/M07_phrases02_01.jpg', 'b'],\n ['b/M07_phrases02_02.jpg', 'b'],\n ['b/M07_phrases02_03.jpg', 'b'],\n ['b/M07_phrases02_04.jpg', 'b'],\n ['b/M07_phrases02_05.jpg', 'b'],\n ['b/M07_phrases02_06.jpg', 'b'],\n ['b/M07_phrases02_07.jpg', 'b'],\n ['b/M07_phrases02_08.jpg', 'b'],\n ['b/M07_phrases02_09.jpg', 'b'],\n ['b/M07_phrases02_10.jpg', 'b'],\n ['b/M08_phrases02_01.jpg', 'b'],\n ['b/M08_phrases02_02.jpg', 'b'],\n ['b/M08_phrases02_03.jpg', 'b'],\n ['b/M08_phrases02_04.jpg', 'b'],\n ['b/M08_phrases02_05.jpg', 'b'],\n ['b/M08_phrases02_06.jpg', 'b'],\n ['b/M08_phrases02_07.jpg', 'b'],\n ['b/M08_phrases02_08.jpg', 'b'],\n ['b/M08_phrases02_09.jpg', 'b'],\n ['b/M08_phrases02_10.jpg', 'b'],\n ['c/F01_phrases03_01.jpg', 'c'],\n ['c/F01_phrases03_02.jpg', 'c'],\n ['c/F01_phrases03_03.jpg', 'c'],\n ['c/F01_phrases03_04.jpg', 'c'],\n ['c/F01_phrases03_05.jpg', 'c'],\n ['c/F01_phrases03_06.jpg', 'c'],\n ['c/F01_phrases03_07.jpg', 'c'],\n ['c/F01_phrases03_08.jpg', 'c'],\n ['c/F01_phrases03_09.jpg', 'c'],\n ['c/F01_phrases03_10.jpg', 'c'],\n ['c/F02_phrases03_01.jpg', 'c'],\n ['c/F02_phrases03_02.jpg', 'c'],\n ['c/F02_phrases03_03.jpg', 'c'],\n ['c/F02_phrases03_04.jpg', 'c'],\n ['c/F02_phrases03_05.jpg', 'c'],\n ['c/F02_phrases03_06.jpg', 'c'],\n ['c/F02_phrases03_07.jpg', 'c'],\n ['c/F02_phrases03_08.jpg', 'c'],\n ['c/F02_phrases03_09.jpg', 'c'],\n ['c/F02_phrases03_10.jpg', 'c'],\n ['c/F05_phrases03_01.jpg', 'c'],\n ['c/F05_phrases03_02.jpg', 'c'],\n ['c/F05_phrases03_03.jpg', 'c'],\n ['c/F05_phrases03_04.jpg', 'c'],\n ['c/F05_phrases03_05.jpg', 'c'],\n ['c/F05_phrases03_06.jpg', 'c'],\n ['c/F05_phrases03_07.jpg', 'c'],\n ['c/F05_phrases03_08.jpg', 'c'],\n ['c/F05_phrases03_09.jpg', 'c'],\n ['c/F05_phrases03_10.jpg', 'c'],\n ['c/F06_phrases03_01.jpg', 'c'],\n ['c/F06_phrases03_02.jpg', 'c'],\n ['c/F06_phrases03_03.jpg', 'c'],\n ['c/F06_phrases03_04.jpg', 'c'],\n ['c/F06_phrases03_05.jpg', 'c'],\n ['c/F06_phrases03_06.jpg', 'c'],\n ['c/F06_phrases03_07.jpg', 'c'],\n ['c/F06_phrases03_08.jpg', 'c'],\n ['c/F06_phrases03_09.jpg', 'c'],\n ['c/F06_phrases03_10.jpg', 'c'],\n ['c/F07_phrases03_01.jpg', 'c'],\n ['c/F07_phrases03_02.jpg', 'c'],\n ['c/F07_phrases03_03.jpg', 'c'],\n ['c/F07_phrases03_04.jpg', 'c'],\n ['c/F07_phrases03_05.jpg', 'c'],\n ['c/F07_phrases03_06.jpg', 'c'],\n ['c/F07_phrases03_07.jpg', 'c'],\n ['c/F07_phrases03_08.jpg', 'c'],\n ['c/F07_phrases03_09.jpg', 'c'],\n ['c/F07_phrases03_10.jpg', 'c'],\n ['c/F08_phrases03_01.jpg', 'c'],\n ['c/F08_phrases03_02.jpg', 'c'],\n ['c/F08_phrases03_03.jpg', 'c'],\n ['c/F08_phrases03_04.jpg', 'c'],\n ['c/F08_phrases03_05.jpg', 'c'],\n ['c/F08_phrases03_06.jpg', 'c'],\n ['c/F08_phrases03_07.jpg', 'c'],\n ['c/F08_phrases03_08.jpg', 'c'],\n ['c/F08_phrases03_09.jpg', 'c'],\n ['c/F08_phrases03_10.jpg', 'c'],\n ['c/F09_phrases03_01.jpg', 'c'],\n ['c/F09_phrases03_02.jpg', 'c'],\n ['c/F09_phrases03_03.jpg', 'c'],\n ['c/F09_phrases03_04.jpg', 'c'],\n ['c/F09_phrases03_05.jpg', 'c'],\n ['c/F09_phrases03_06.jpg', 'c'],\n ['c/F09_phrases03_07.jpg', 'c'],\n ['c/F09_phrases03_08.jpg', 'c'],\n ['c/F09_phrases03_09.jpg', 'c'],\n ['c/F09_phrases03_10.jpg', 'c'],\n ['c/F10_phrases03_01.jpg', 'c'],\n ['c/F10_phrases03_02.jpg', 'c'],\n ['c/F10_phrases03_03.jpg', 'c'],\n ['c/F10_phrases03_04.jpg', 'c'],\n ['c/F10_phrases03_05.jpg', 'c'],\n ['c/F10_phrases03_06.jpg', 'c'],\n ['c/F10_phrases03_07.jpg', 'c'],\n ['c/F10_phrases03_08.jpg', 'c'],\n ['c/F10_phrases03_09.jpg', 'c'],\n ['c/F10_phrases03_10.jpg', 'c'],\n ['c/F11_phrases03_01.jpg', 'c'],\n ['c/F11_phrases03_02.jpg', 'c'],\n ['c/F11_phrases03_03.jpg', 'c'],\n ['c/F11_phrases03_04.jpg', 'c'],\n ['c/F11_phrases03_05.jpg', 'c'],\n ['c/F11_phrases03_06.jpg', 'c'],\n ['c/F11_phrases03_07.jpg', 'c'],\n ['c/F11_phrases03_08.jpg', 'c'],\n ['c/F11_phrases03_09.jpg', 'c'],\n ['c/F11_phrases03_10.jpg', 'c'],\n ['c/M01_phrases03_01.jpg', 'c'],\n ['c/M01_phrases03_02.jpg', 'c'],\n ['c/M01_phrases03_03.jpg', 'c'],\n ['c/M01_phrases03_04.jpg', 'c'],\n ['c/M01_phrases03_05.jpg', 'c'],\n ['c/M01_phrases03_06.jpg', 'c'],\n ['c/M01_phrases03_07.jpg', 'c'],\n ['c/M01_phrases03_08.jpg', 'c'],\n ['c/M01_phrases03_09.jpg', 'c'],\n ['c/M01_phrases03_10.jpg', 'c'],\n ['c/M02_phrases03_01.jpg', 'c'],\n ['c/M02_phrases03_02.jpg', 'c'],\n ['c/M02_phrases03_03.jpg', 'c'],\n ['c/M02_phrases03_04.jpg', 'c'],\n ['c/M02_phrases03_05.jpg', 'c'],\n ['c/M02_phrases03_06.jpg', 'c'],\n ['c/M02_phrases03_07.jpg', 'c'],\n ['c/M04_phrases03_01.jpg', 'c'],\n ['c/M04_phrases03_02.jpg', 'c'],\n ['c/M04_phrases03_03.jpg', 'c'],\n ['c/M04_phrases03_04.jpg', 'c'],\n ['c/M04_phrases03_05.jpg', 'c'],\n ['c/M04_phrases03_06.jpg', 'c'],\n ['c/M04_phrases03_07.jpg', 'c'],\n ['c/M04_phrases03_08.jpg', 'c'],\n ['c/M04_phrases03_09.jpg', 'c'],\n ['c/M04_phrases03_10.jpg', 'c'],\n ['c/M07_phrases03_01.jpg', 'c'],\n ['c/M07_phrases03_02.jpg', 'c'],\n ['c/M07_phrases03_03.jpg', 'c'],\n ['c/M07_phrases03_04.jpg', 'c'],\n ['c/M07_phrases03_05.jpg', 'c'],\n ['c/M07_phrases03_06.jpg', 'c'],\n ['c/M07_phrases03_07.jpg', 'c'],\n ['c/M07_phrases03_08.jpg', 'c'],\n ['c/M08_phrases03_06.jpg', 'c'],\n ['c/M08_phrases03_07.jpg', 'c'],\n ['c/M08_phrases03_08.jpg', 'c'],\n ['c/M08_phrases03_09.jpg', 'c'],\n ['c/M08_phrases03_10.jpg', 'c'],\n ['d/F01_phrases04_01.jpg', 'd'],\n ['d/F01_phrases04_02.jpg', 'd'],\n ['d/F01_phrases04_03.jpg', 'd'],\n ['d/F01_phrases04_04.jpg', 'd'],\n ['d/F01_phrases04_05.jpg', 'd'],\n ['d/F01_phrases04_06.jpg', 'd'],\n ['d/F01_phrases04_07.jpg', 'd'],\n ['d/F01_phrases04_08.jpg', 'd'],\n ['d/F01_phrases04_09.jpg', 'd'],\n ['d/F01_phrases04_10.jpg', 'd'],\n ['d/F02_phrases04_01.jpg', 'd'],\n ['d/F02_phrases04_02.jpg', 'd'],\n ['d/F02_phrases04_03.jpg', 'd'],\n ['d/F02_phrases04_04.jpg', 'd'],\n ['d/F02_phrases04_05.jpg', 'd'],\n ['d/F02_phrases04_06.jpg', 'd'],\n ['d/F02_phrases04_07.jpg', 'd'],\n ['d/F02_phrases04_08.jpg', 'd'],\n ['d/F02_phrases04_09.jpg', 'd'],\n ['d/F02_phrases04_10.jpg', 'd'],\n ['d/F04_phrases04_01.jpg', 'd'],\n ['d/F04_phrases04_02.jpg', 'd'],\n ['d/F04_phrases04_03.jpg', 'd'],\n ['d/F04_phrases04_04.jpg', 'd'],\n ['d/F04_phrases04_05.jpg', 'd'],\n ['d/F04_phrases04_06.jpg', 'd'],\n ['d/F04_phrases04_07.jpg', 'd'],\n ['d/F04_phrases04_08.jpg', 'd'],\n ['d/F04_phrases04_09.jpg', 'd'],\n ['d/F04_phrases04_10.jpg', 'd'],\n ['d/F05_phrases04_01.jpg', 'd'],\n ['d/F05_phrases04_02.jpg', 'd'],\n ['d/F05_phrases04_03.jpg', 'd'],\n ['d/F05_phrases04_04.jpg', 'd'],\n ['d/F05_phrases04_05.jpg', 'd'],\n ['d/F05_phrases04_06.jpg', 'd'],\n ['d/F05_phrases04_07.jpg', 'd'],\n ['d/F05_phrases04_08.jpg', 'd'],\n ['d/F05_phrases04_09.jpg', 'd'],\n ['d/F05_phrases04_10.jpg', 'd'],\n ['d/F06_phrases04_01.jpg', 'd'],\n ['d/F06_phrases04_02.jpg', 'd'],\n ['d/F06_phrases04_03.jpg', 'd'],\n ['d/F06_phrases04_04.jpg', 'd'],\n ['d/F06_phrases04_05.jpg', 'd'],\n ['d/F06_phrases04_06.jpg', 'd'],\n ['d/F06_phrases04_07.jpg', 'd'],\n ['d/F06_phrases04_08.jpg', 'd'],\n ['d/F06_phrases04_09.jpg', 'd'],\n ['d/F06_phrases04_10.jpg', 'd'],\n ['d/F07_phrases04_01.jpg', 'd'],\n ['d/F07_phrases04_02.jpg', 'd'],\n ['d/F07_phrases04_03.jpg', 'd'],\n ['d/F07_phrases04_04.jpg', 'd'],\n ['d/F07_phrases04_05.jpg', 'd'],\n ['d/F07_phrases04_06.jpg', 'd'],\n ['d/F07_phrases04_07.jpg', 'd'],\n ['d/F07_phrases04_08.jpg', 'd'],\n ['d/F07_phrases04_09.jpg', 'd'],\n ['d/F07_phrases04_10.jpg', 'd'],\n ['d/F08_phrases04_01.jpg', 'd'],\n ['d/F08_phrases04_02.jpg', 'd'],\n ['d/F08_phrases04_03.jpg', 'd'],\n ['d/F08_phrases04_04.jpg', 'd'],\n ['d/F08_phrases04_05.jpg', 'd'],\n ['d/F08_phrases04_06.jpg', 'd'],\n ['d/F08_phrases04_07.jpg', 'd'],\n ['d/F08_phrases04_08.jpg', 'd'],\n ['d/F08_phrases04_09.jpg', 'd'],\n ['d/F08_phrases04_10.jpg', 'd'],\n ['d/F09_phrases04_01.jpg', 'd'],\n ['d/F09_phrases04_02.jpg', 'd'],\n ['d/F09_phrases04_03.jpg', 'd'],\n ['d/F09_phrases04_04.jpg', 'd'],\n ['d/F09_phrases04_05.jpg', 'd'],\n ['d/F09_phrases04_06.jpg', 'd'],\n ['d/F09_phrases04_07.jpg', 'd'],\n ['d/F09_phrases04_08.jpg', 'd'],\n ['d/F09_phrases04_09.jpg', 'd'],\n ['d/F09_phrases04_10.jpg', 'd'],\n ['d/F10_phrases04_01.jpg', 'd'],\n ['d/F10_phrases04_02.jpg', 'd'],\n ['d/F10_phrases04_03.jpg', 'd'],\n ['d/F10_phrases04_04.jpg', 'd'],\n ['d/F10_phrases04_05.jpg', 'd'],\n ['d/F10_phrases04_06.jpg', 'd'],\n ['d/F10_phrases04_07.jpg', 'd'],\n ['d/F10_phrases04_08.jpg', 'd'],\n ['d/F10_phrases04_09.jpg', 'd'],\n ['d/F10_phrases04_10.jpg', 'd'],\n ['d/F11_phrases04_01.jpg', 'd'],\n ['d/F11_phrases04_02.jpg', 'd'],\n ['d/F11_phrases04_03.jpg', 'd'],\n ['d/F11_phrases04_04.jpg', 'd'],\n ['d/F11_phrases04_05.jpg', 'd'],\n ['d/F11_phrases04_06.jpg', 'd'],\n ['d/F11_phrases04_07.jpg', 'd'],\n ['d/F11_phrases04_08.jpg', 'd'],\n ['d/F11_phrases04_09.jpg', 'd'],\n ['d/F11_phrases04_10.jpg', 'd'],\n ['d/M01_phrases04_01.jpg', 'd'],\n ['d/M01_phrases04_02.jpg', 'd'],\n ['d/M01_phrases04_03.jpg', 'd'],\n ['d/M01_phrases04_04.jpg', 'd'],\n ['d/M01_phrases04_05.jpg', 'd'],\n ['d/M01_phrases04_06.jpg', 'd'],\n ['d/M01_phrases04_07.jpg', 'd'],\n ['d/M01_phrases04_08.jpg', 'd'],\n ['d/M01_phrases04_09.jpg', 'd'],\n ['d/M01_phrases04_10.jpg', 'd'],\n ['d/M02_phrases04_01.jpg', 'd'],\n ['d/M02_phrases04_02.jpg', 'd'],\n ['d/M02_phrases04_03.jpg', 'd'],\n ['d/M02_phrases04_04.jpg', 'd'],\n ['d/M02_phrases04_05.jpg', 'd'],\n ['d/M02_phrases04_06.jpg', 'd'],\n ['d/M02_phrases04_07.jpg', 'd'],\n ['d/M02_phrases04_08.jpg', 'd'],\n ['d/M02_phrases04_09.jpg', 'd'],\n ['d/M02_phrases04_10.jpg', 'd'],\n ['d/M04_phrases04_01.jpg', 'd'],\n ['d/M04_phrases04_02.jpg', 'd'],\n ['d/M04_phrases04_03.jpg', 'd'],\n ['d/M04_phrases04_04.jpg', 'd'],\n ['d/M04_phrases04_05.jpg', 'd'],\n ['d/M04_phrases04_06.jpg', 'd'],\n ['d/M08_phrases04_07.jpg', 'd'],\n ['d/M08_phrases04_08.jpg', 'd'],\n ['d/M08_phrases04_09.jpg', 'd'],\n ['d/M08_phrases04_10.jpg', 'd'],\n ['e/F01_phrases05_01.jpg', 'e'],\n ['e/F01_phrases05_02.jpg', 'e'],\n ['e/F01_phrases05_03.jpg', 'e'],\n ['e/F01_phrases05_04.jpg', 'e'],\n ['e/F01_phrases05_05.jpg', 'e'],\n ['e/F01_phrases05_06.jpg', 'e'],\n ['e/F01_phrases05_07.jpg', 'e'],\n ['e/F01_phrases05_08.jpg', 'e'],\n ['e/F01_phrases05_09.jpg', 'e'],\n ['e/F01_phrases05_10.jpg', 'e'],\n ['e/F02_phrases05_01.jpg', 'e'],\n ['e/F02_phrases05_02.jpg', 'e'],\n ['e/F02_phrases05_03.jpg', 'e'],\n ['e/F02_phrases05_04.jpg', 'e'],\n ['e/F04_phrases05_05.jpg', 'e'],\n ['e/F04_phrases05_06.jpg', 'e'],\n ['e/F04_phrases05_10.jpg', 'e'],\n ['e/F05_phrases05_01.jpg', 'e'],\n ['e/F05_phrases05_02.jpg', 'e'],\n ['e/F05_phrases05_03.jpg', 'e'],\n ['e/F05_phrases05_04.jpg', 'e'],\n ['e/F05_phrases05_05.jpg', 'e'],\n ['e/F05_phrases05_06.jpg', 'e'],\n ['e/F05_phrases05_07.jpg', 'e'],\n ['e/F05_phrases05_08.jpg', 'e'],\n ['e/F06_phrases05_06.jpg', 'e'],\n ['e/F06_phrases05_07.jpg', 'e'],\n ['e/F06_phrases05_08.jpg', 'e'],\n ['e/F06_phrases05_09.jpg', 'e'],\n ['e/F06_phrases05_10.jpg', 'e'],\n ['e/F07_phrases05_01.jpg', 'e'],\n ['e/F07_phrases05_02.jpg', 'e'],\n ['e/F07_phrases05_03.jpg', 'e'],\n ['e/F07_phrases05_04.jpg', 'e'],\n ['e/F07_phrases05_05.jpg', 'e'],\n ['e/F07_phrases05_06.jpg', 'e'],\n ['e/F07_phrases05_07.jpg', 'e'],\n ['e/F07_phrases05_08.jpg', 'e'],\n ['e/F07_phrases05_09.jpg', 'e'],\n ['e/F07_phrases05_10.jpg', 'e'],\n ['e/F08_phrases05_01.jpg', 'e'],\n ['e/F08_phrases05_02.jpg', 'e'],\n ['e/F08_phrases05_03.jpg', 'e'],\n ['e/F08_phrases05_04.jpg', 'e'],\n ['e/F08_phrases05_05.jpg', 'e'],\n ['e/F08_phrases05_06.jpg', 'e'],\n ['e/F08_phrases05_07.jpg', 'e'],\n ['e/F08_phrases05_08.jpg', 'e'],\n ['e/F08_phrases05_09.jpg', 'e'],\n ['e/F08_phrases05_10.jpg', 'e'],\n ['e/F09_phrases05_01.jpg', 'e'],\n ['e/F09_phrases05_02.jpg', 'e'],\n ['e/F09_phrases05_03.jpg', 'e'],\n ['e/F09_phrases05_04.jpg', 'e'],\n ['e/F09_phrases05_05.jpg', 'e'],\n ['e/F09_phrases05_06.jpg', 'e'],\n ['e/F09_phrases05_07.jpg', 'e'],\n ['e/F09_phrases05_08.jpg', 'e'],\n ['e/F09_phrases05_09.jpg', 'e'],\n ['e/F09_phrases05_10.jpg', 'e'],\n ['e/F10_phrases05_01.jpg', 'e'],\n ['e/F10_phrases05_02.jpg', 'e'],\n ['e/F10_phrases05_03.jpg', 'e'],\n ['e/F10_phrases05_04.jpg', 'e'],\n ['e/F10_phrases05_05.jpg', 'e'],\n ['e/F10_phrases05_06.jpg', 'e'],\n ['e/F10_phrases05_07.jpg', 'e'],\n ['e/F10_phrases05_08.jpg', 'e'],\n ['e/F10_phrases05_09.jpg', 'e'],\n ['e/F10_phrases05_10.jpg', 'e'],\n ['e/F11_phrases05_01.jpg', 'e'],\n ['e/F11_phrases05_02.jpg', 'e'],\n ['e/F11_phrases05_03.jpg', 'e'],\n ['e/F11_phrases05_04.jpg', 'e'],\n ['e/F11_phrases05_05.jpg', 'e'],\n ['e/F11_phrases05_06.jpg', 'e'],\n ['e/F11_phrases05_07.jpg', 'e'],\n ['e/F11_phrases05_08.jpg', 'e'],\n ['e/F11_phrases05_09.jpg', 'e'],\n ['e/F11_phrases05_10.jpg', 'e'],\n ['e/M01_phrases05_01.jpg', 'e'],\n ['e/M01_phrases05_02.jpg', 'e'],\n ['e/M01_phrases05_03.jpg', 'e'],\n ['e/M01_phrases05_04.jpg', 'e'],\n ['e/M01_phrases05_05.jpg', 'e'],\n ['e/M01_phrases05_06.jpg', 'e'],\n ['e/M01_phrases05_07.jpg', 'e'],\n ['e/M01_phrases05_08.jpg', 'e'],\n ['e/M01_phrases05_09.jpg', 'e'],\n ['e/M01_phrases05_10.jpg', 'e'],\n ['e/M02_phrases05_01.jpg', 'e'],\n ['e/M02_phrases05_02.jpg', 'e'],\n ['e/M02_phrases05_03.jpg', 'e'],\n ['e/M02_phrases05_04.jpg', 'e'],\n ['e/M02_phrases05_05.jpg', 'e'],\n ['e/M02_phrases05_06.jpg', 'e'],\n ['e/M02_phrases05_07.jpg', 'e'],\n ['e/M02_phrases05_08.jpg', 'e'],\n ['e/M02_phrases05_09.jpg', 'e'],\n ['e/M02_phrases05_10.jpg', 'e'],\n ['e/M04_phrases05_01.jpg', 'e'],\n ['e/M04_phrases05_02.jpg', 'e'],\n ['e/M04_phrases05_03.jpg', 'e'],\n ['e/M04_phrases05_04.jpg', 'e'],\n ['e/M04_phrases05_05.jpg', 'e'],\n ['e/M04_phrases05_06.jpg', 'e'],\n ['e/M04_phrases05_07.jpg', 'e'],\n ['e/M04_phrases05_08.jpg', 'e'],\n ['e/M04_phrases05_09.jpg', 'e'],\n ['e/M04_phrases05_10.jpg', 'e'],\n ['e/M07_phrases05_01.jpg', 'e'],\n ['e/M07_phrases05_02.jpg', 'e'],\n ['e/M07_phrases05_03.jpg', 'e'],\n ['e/M07_phrases05_04.jpg', 'e'],\n ['e/M07_phrases05_05.jpg', 'e'],\n ['e/M07_phrases05_06.jpg', 'e'],\n ['e/M07_phrases05_07.jpg', 'e'],\n ['e/M07_phrases05_08.jpg', 'e'],\n ['e/M07_phrases05_09.jpg', 'e'],\n ['e/M07_phrases05_10.jpg', 'e'],\n ['e/M08_phrases05_01.jpg', 'e'],\n ['e/M08_phrases05_02.jpg', 'e'],\n ['e/M08_phrases05_03.jpg', 'e'],\n ['e/M08_phrases05_04.jpg', 'e'],\n ['e/M08_phrases05_05.jpg', 'e'],\n ['e/M08_phrases05_06.jpg', 'e'],\n ['e/M08_phrases05_07.jpg', 'e'],\n ['e/M08_phrases05_08.jpg', 'e'],\n ['e/M08_phrases05_09.jpg', 'e'],\n ['e/M08_phrases05_10.jpg', 'e'],\n ['f/F02_phrases06_01.jpg', 'f'],\n ['f/F02_phrases06_02.jpg', 'f'],\n ['f/F02_phrases06_03.jpg', 'f'],\n ['f/F02_phrases06_04.jpg', 'f'],\n ['f/F02_phrases06_05.jpg', 'f'],\n ['f/F02_phrases06_06.jpg', 'f'],\n ['f/F02_phrases06_07.jpg', 'f'],\n ['f/F02_phrases06_08.jpg', 'f'],\n ['f/F02_phrases06_09.jpg', 'f'],\n ['f/F02_phrases06_10.jpg', 'f'],\n ['f/F04_phrases06_01.jpg', 'f'],\n ['f/F04_phrases06_02.jpg', 'f'],\n ['f/F04_phrases06_03.jpg', 'f'],\n ['f/F04_phrases06_04.jpg', 'f'],\n ['f/F04_phrases06_05.jpg', 'f'],\n ['f/F04_phrases06_06.jpg', 'f'],\n ['f/F05_phrases06_01.jpg', 'f'],\n ['f/F05_phrases06_02.jpg', 'f'],\n ['f/F05_phrases06_03.jpg', 'f'],\n ['f/F05_phrases06_04.jpg', 'f'],\n ['f/F05_phrases06_05.jpg', 'f'],\n ['f/F05_phrases06_06.jpg', 'f'],\n ['f/F05_phrases06_07.jpg', 'f'],\n ['f/F05_phrases06_08.jpg', 'f'],\n ['f/F05_phrases06_09.jpg', 'f'],\n ['f/F05_phrases06_10.jpg', 'f'],\n ['f/F06_phrases06_01.jpg', 'f'],\n ['f/F06_phrases06_02.jpg', 'f'],\n ['f/F06_phrases06_03.jpg', 'f'],\n ['f/F06_phrases06_07.jpg', 'f'],\n ['f/F06_phrases06_08.jpg', 'f'],\n ['f/F06_phrases06_09.jpg', 'f'],\n ['f/F06_phrases06_10.jpg', 'f'],\n ['f/F07_phrases06_01.jpg', 'f'],\n ['f/F07_phrases06_02.jpg', 'f'],\n ['f/F07_phrases06_03.jpg', 'f'],\n ['f/F07_phrases06_04.jpg', 'f'],\n ['f/F07_phrases06_05.jpg', 'f'],\n ['f/F07_phrases06_06.jpg', 'f'],\n ['f/F07_phrases06_07.jpg', 'f'],\n ['f/F07_phrases06_08.jpg', 'f'],\n ['f/F07_phrases06_09.jpg', 'f'],\n ['f/F07_phrases06_10.jpg', 'f'],\n ['f/F08_phrases06_04.jpg', 'f'],\n ['f/F08_phrases06_05.jpg', 'f'],\n ['f/F08_phrases06_06.jpg', 'f'],\n ['f/F08_phrases06_07.jpg', 'f'],\n ['f/F08_phrases06_08.jpg', 'f'],\n ['f/F08_phrases06_09.jpg', 'f'],\n ['f/F08_phrases06_10.jpg', 'f'],\n ['f/F09_phrases06_01.jpg', 'f'],\n ['f/F09_phrases06_02.jpg', 'f'],\n ['f/F09_phrases06_03.jpg', 'f'],\n ['f/F09_phrases06_04.jpg', 'f'],\n ['f/F09_phrases06_05.jpg', 'f'],\n ['f/F09_phrases06_06.jpg', 'f'],\n ['f/F09_phrases06_07.jpg', 'f'],\n ['f/F09_phrases06_08.jpg', 'f'],\n ['f/F09_phrases06_09.jpg', 'f'],\n ['f/F09_phrases06_10.jpg', 'f'],\n ['f/F10_phrases06_01.jpg', 'f'],\n ['f/F10_phrases06_02.jpg', 'f'],\n ['f/F10_phrases06_03.jpg', 'f'],\n ['f/F10_phrases06_04.jpg', 'f'],\n ['f/F10_phrases06_05.jpg', 'f'],\n ['f/F10_phrases06_06.jpg', 'f'],\n ['f/F10_phrases06_07.jpg', 'f'],\n ['f/F10_phrases06_08.jpg', 'f'],\n ['f/F10_phrases06_09.jpg', 'f'],\n ['f/F10_phrases06_10.jpg', 'f'],\n ['f/F11_phrases06_01.jpg', 'f'],\n ['f/F11_phrases06_02.jpg', 'f'],\n ['f/F11_phrases06_03.jpg', 'f'],\n ['f/F11_phrases06_04.jpg', 'f'],\n ['f/F11_phrases06_05.jpg', 'f'],\n ['f/F11_phrases06_06.jpg', 'f'],\n ['f/F11_phrases06_07.jpg', 'f'],\n ['f/F11_phrases06_08.jpg', 'f'],\n ['f/F11_phrases06_09.jpg', 'f'],\n ['f/F11_phrases06_10.jpg', 'f'],\n ['f/M01_phrases06_01.jpg', 'f'],\n ['f/M01_phrases06_02.jpg', 'f'],\n ['f/M01_phrases06_03.jpg', 'f'],\n ['f/M01_phrases06_04.jpg', 'f'],\n ['f/M01_phrases06_05.jpg', 'f'],\n ['f/M01_phrases06_06.jpg', 'f'],\n ['f/M01_phrases06_07.jpg', 'f'],\n ['f/M01_phrases06_08.jpg', 'f'],\n ['f/M01_phrases06_09.jpg', 'f'],\n ['f/M01_phrases06_10.jpg', 'f'],\n ['f/M02_phrases06_01.jpg', 'f'],\n ['f/M02_phrases06_02.jpg', 'f'],\n ['f/M02_phrases06_03.jpg', 'f'],\n ['f/M02_phrases06_04.jpg', 'f'],\n ['f/M02_phrases06_05.jpg', 'f'],\n ['f/M02_phrases06_06.jpg', 'f'],\n ['f/M02_phrases06_07.jpg', 'f'],\n ['f/M02_phrases06_08.jpg', 'f'],\n ['f/M02_phrases06_09.jpg', 'f'],\n ['f/M02_phrases06_10.jpg', 'f'],\n ['f/M04_phrases06_01.jpg', 'f'],\n ['f/M04_phrases06_02.jpg', 'f'],\n ['f/M04_phrases06_03.jpg', 'f'],\n ['f/M04_phrases06_04.jpg', 'f'],\n ['f/M04_phrases06_05.jpg', 'f'],\n ['f/M04_phrases06_06.jpg', 'f'],\n ['f/M04_phrases06_07.jpg', 'f'],\n ['f/M04_phrases06_08.jpg', 'f'],\n ['f/M04_phrases06_09.jpg', 'f'],\n ['f/M04_phrases06_10.jpg', 'f'],\n ['f/M07_phrases06_01.jpg', 'f'],\n ['f/M07_phrases06_02.jpg', 'f'],\n ['f/M07_phrases06_03.jpg', 'f'],\n ['f/M07_phrases06_04.jpg', 'f'],\n ['f/M07_phrases06_05.jpg', 'f'],\n ['f/M07_phrases06_06.jpg', 'f'],\n ['f/M07_phrases06_07.jpg', 'f'],\n ['f/M07_phrases06_08.jpg', 'f'],\n ['f/M07_phrases06_09.jpg', 'f'],\n ['f/M07_phrases06_10.jpg', 'f'],\n ['f/M08_phrases06_01.jpg', 'f'],\n ['f/M08_phrases06_02.jpg', 'f'],\n ['f/M08_phrases06_03.jpg', 'f'],\n ['f/M08_phrases06_04.jpg', 'f'],\n ['f/M08_phrases06_05.jpg', 'f'],\n ['f/M08_phrases06_06.jpg', 'f'],\n ['f/M08_phrases06_07.jpg', 'f'],\n ['f/M08_phrases06_08.jpg', 'f'],\n ['f/M08_phrases06_09.jpg', 'f'],\n ['f/M08_phrases06_10.jpg', 'f'],\n ['g/F01_phrases07_01.jpg', 'g'],\n ['g/F01_phrases07_02.jpg', 'g'],\n ['g/F01_phrases07_03.jpg', 'g'],\n ['g/F01_phrases07_04.jpg', 'g'],\n ['g/F01_phrases07_05.jpg', 'g'],\n ['g/F01_phrases07_06.jpg', 'g'],\n ['g/F01_phrases07_07.jpg', 'g'],\n ['g/F01_phrases07_08.jpg', 'g'],\n ['g/F01_phrases07_09.jpg', 'g'],\n ['g/F01_phrases07_10.jpg', 'g'],\n ['g/F02_phrases07_01.jpg', 'g'],\n ['g/F02_phrases07_02.jpg', 'g'],\n ['g/F02_phrases07_03.jpg', 'g'],\n ['g/F02_phrases07_04.jpg', 'g'],\n ['g/F02_phrases07_05.jpg', 'g'],\n ['g/F02_phrases07_06.jpg', 'g'],\n ['g/F02_phrases07_07.jpg', 'g'],\n ['g/F02_phrases07_08.jpg', 'g'],\n ['g/F02_phrases07_09.jpg', 'g'],\n ['g/F02_phrases07_10.jpg', 'g'],\n ['g/F04_phrases07_01.jpg', 'g'],\n ['g/F04_phrases07_02.jpg', 'g'],\n ['g/F04_phrases07_03.jpg', 'g'],\n ['g/F04_phrases07_04.jpg', 'g'],\n ['g/F04_phrases07_05.jpg', 'g'],\n ['g/F04_phrases07_06.jpg', 'g'],\n ['g/F04_phrases07_07.jpg', 'g'],\n ['g/F04_phrases07_08.jpg', 'g'],\n ['g/F04_phrases07_09.jpg', 'g'],\n ['g/F04_phrases07_10.jpg', 'g'],\n ['g/F05_phrases07_01.jpg', 'g'],\n ['g/F05_phrases07_02.jpg', 'g'],\n ['g/F05_phrases07_03.jpg', 'g'],\n ['g/F05_phrases07_04.jpg', 'g'],\n ['g/F05_phrases07_05.jpg', 'g'],\n ['g/F05_phrases07_06.jpg', 'g'],\n ['g/F05_phrases07_07.jpg', 'g'],\n ['g/F05_phrases07_08.jpg', 'g'],\n ['g/F05_phrases07_09.jpg', 'g'],\n ['g/F05_phrases07_10.jpg', 'g'],\n ['g/F06_phrases07_01.jpg', 'g'],\n ['g/F06_phrases07_02.jpg', 'g'],\n ['g/F06_phrases07_03.jpg', 'g'],\n ['g/F06_phrases07_04.jpg', 'g'],\n ['g/F06_phrases07_05.jpg', 'g'],\n ['g/F06_phrases07_06.jpg', 'g'],\n ['g/F06_phrases07_07.jpg', 'g'],\n ['g/F06_phrases07_08.jpg', 'g'],\n ['g/F06_phrases07_09.jpg', 'g'],\n ['g/F06_phrases07_10.jpg', 'g'],\n ['g/F07_phrases07_01.jpg', 'g'],\n ['g/F07_phrases07_02.jpg', 'g'],\n ['g/F07_phrases07_03.jpg', 'g'],\n ['g/F07_phrases07_04.jpg', 'g'],\n ['g/F07_phrases07_05.jpg', 'g'],\n ['g/F07_phrases07_06.jpg', 'g'],\n ['g/F07_phrases07_07.jpg', 'g'],\n ['g/F07_phrases07_08.jpg', 'g'],\n ['g/F07_phrases07_09.jpg', 'g'],\n ['g/F07_phrases07_10.jpg', 'g'],\n ['g/F08_phrases07_01.jpg', 'g'],\n ['g/F08_phrases07_02.jpg', 'g'],\n ['g/F08_phrases07_03.jpg', 'g'],\n ['g/F08_phrases07_04.jpg', 'g'],\n ['g/F08_phrases07_05.jpg', 'g'],\n ['g/F08_phrases07_06.jpg', 'g'],\n ['g/F08_phrases07_08.jpg', 'g'],\n ['g/F08_phrases07_09.jpg', 'g'],\n ['g/F08_phrases07_10.jpg', 'g'],\n ['g/F09_phrases07_01.jpg', 'g'],\n ['g/F09_phrases07_02.jpg', 'g'],\n ['g/F09_phrases07_03.jpg', 'g'],\n ['g/F09_phrases07_05.jpg', 'g'],\n ['g/F09_phrases07_06.jpg', 'g'],\n ['g/F09_phrases07_07.jpg', 'g'],\n ['g/F09_phrases07_08.jpg', 'g'],\n ['g/F09_phrases07_09.jpg', 'g'],\n ['g/F09_phrases07_10.jpg', 'g'],\n ['g/F10_phrases07_01.jpg', 'g'],\n ['g/F10_phrases07_02.jpg', 'g'],\n ['g/F10_phrases07_03.jpg', 'g'],\n ['g/F10_phrases07_04.jpg', 'g'],\n ['g/F10_phrases07_05.jpg', 'g'],\n ['g/F10_phrases07_06.jpg', 'g'],\n ['g/F10_phrases07_10.jpg', 'g'],\n ['g/F11_phrases07_01.jpg', 'g'],\n ['g/F11_phrases07_04.jpg', 'g'],\n ['g/F11_phrases07_05.jpg', 'g'],\n ['g/F11_phrases07_06.jpg', 'g'],\n ['g/F11_phrases07_07.jpg', 'g'],\n ['g/F11_phrases07_08.jpg', 'g'],\n ['g/F11_phrases07_09.jpg', 'g'],\n ['g/F11_phrases07_10.jpg', 'g'],\n ['g/M01_phrases07_02.jpg', 'g'],\n ['g/M01_phrases07_03.jpg', 'g'],\n ['g/M01_phrases07_04.jpg', 'g'],\n ['g/M01_phrases07_05.jpg', 'g'],\n ['g/M01_phrases07_08.jpg', 'g'],\n ['g/M01_phrases07_09.jpg', 'g'],\n ['g/M01_phrases07_10.jpg', 'g'],\n ['g/M04_phrases07_01.jpg', 'g'],\n ['g/M04_phrases07_02.jpg', 'g'],\n ['g/M04_phrases07_03.jpg', 'g'],\n ['g/M04_phrases07_04.jpg', 'g'],\n ['g/M04_phrases07_05.jpg', 'g'],\n ['g/M04_phrases07_06.jpg', 'g'],\n ['g/M04_phrases07_07.jpg', 'g'],\n ['g/M04_phrases07_08.jpg', 'g'],\n ['g/M04_phrases07_09.jpg', 'g'],\n ['g/M04_phrases07_10.jpg', 'g'],\n ['g/M07_phrases07_01.jpg', 'g'],\n ['g/M07_phrases07_02.jpg', 'g'],\n ['g/M07_phrases07_03.jpg', 'g'],\n ['g/M07_phrases07_04.jpg', 'g'],\n ['g/M07_phrases07_05.jpg', 'g'],\n ['g/M07_phrases07_06.jpg', 'g'],\n ['g/M07_phrases07_07.jpg', 'g'],\n ['g/M07_phrases07_08.jpg', 'g'],\n ['g/M07_phrases07_09.jpg', 'g'],\n ['g/M07_phrases07_10.jpg', 'g'],\n ['g/M08_phrases07_01.jpg', 'g'],\n ['g/M08_phrases07_02.jpg', 'g'],\n ['g/M08_phrases07_03.jpg', 'g'],\n ['g/M08_phrases07_04.jpg', 'g'],\n ['g/M08_phrases07_05.jpg', 'g'],\n ['g/M08_phrases07_06.jpg', 'g'],\n ['g/M08_phrases07_07.jpg', 'g'],\n ['g/M08_phrases07_08.jpg', 'g'],\n ['g/M08_phrases07_09.jpg', 'g'],\n ['g/M08_phrases07_10.jpg', 'g'],\n ['h/F01_phrases08_01.jpg', 'h'],\n ['h/F01_phrases08_02.jpg', 'h'],\n ['h/F01_phrases08_03.jpg', 'h'],\n ['h/F01_phrases08_04.jpg', 'h'],\n ['h/F01_phrases08_05.jpg', 'h'],\n ['h/F01_phrases08_06.jpg', 'h'],\n ['h/F01_phrases08_07.jpg', 'h'],\n ['h/F01_phrases08_08.jpg', 'h'],\n ['h/F01_phrases08_09.jpg', 'h'],\n ['h/F02_phrases08_02.jpg', 'h'],\n ['h/F02_phrases08_03.jpg', 'h'],\n ['h/F02_phrases08_04.jpg', 'h'],\n ['h/F02_phrases08_05.jpg', 'h'],\n ['h/F02_phrases08_06.jpg', 'h'],\n ['h/F02_phrases08_07.jpg', 'h'],\n ['h/F02_phrases08_08.jpg', 'h'],\n ['h/F02_phrases08_09.jpg', 'h'],\n ['h/F02_phrases08_10.jpg', 'h'],\n ['h/F04_phrases08_01.jpg', 'h'],\n ['h/F04_phrases08_02.jpg', 'h'],\n ['h/F04_phrases08_03.jpg', 'h'],\n ['h/F04_phrases08_04.jpg', 'h'],\n ['h/F04_phrases08_05.jpg', 'h'],\n ['h/F04_phrases08_06.jpg', 'h'],\n ['h/F04_phrases08_07.jpg', 'h'],\n ['h/F04_phrases08_08.jpg', 'h'],\n ['h/F04_phrases08_09.jpg', 'h'],\n ['h/F04_phrases08_10.jpg', 'h'],\n ['h/F06_phrases08_01.jpg', 'h'],\n ['h/F06_phrases08_02.jpg', 'h'],\n ['h/F06_phrases08_06.jpg', 'h'],\n ['h/F06_phrases08_07.jpg', 'h'],\n ['h/F06_phrases08_08.jpg', 'h'],\n ['h/F06_phrases08_09.jpg', 'h'],\n ['h/F06_phrases08_10.jpg', 'h'],\n ['h/F07_phrases08_01.jpg', 'h'],\n ['h/F07_phrases08_02.jpg', 'h'],\n ['h/F07_phrases08_03.jpg', 'h'],\n ['h/F07_phrases08_04.jpg', 'h'],\n ['h/F07_phrases08_05.jpg', 'h'],\n ['h/F08_phrases08_01.jpg', 'h'],\n ['h/F08_phrases08_02.jpg', 'h'],\n ['h/F08_phrases08_03.jpg', 'h'],\n ['h/F08_phrases08_04.jpg', 'h'],\n ['h/F08_phrases08_05.jpg', 'h'],\n ['h/F08_phrases08_06.jpg', 'h'],\n ['h/F08_phrases08_07.jpg', 'h'],\n ['h/F08_phrases08_08.jpg', 'h'],\n ['h/F08_phrases08_09.jpg', 'h'],\n ['h/F08_phrases08_10.jpg', 'h'],\n ['h/F09_phrases08_01.jpg', 'h'],\n ['h/F09_phrases08_02.jpg', 'h'],\n ['h/F09_phrases08_03.jpg', 'h'],\n ['h/F09_phrases08_04.jpg', 'h'],\n ['h/F09_phrases08_05.jpg', 'h'],\n ['h/F09_phrases08_06.jpg', 'h'],\n ['h/F09_phrases08_07.jpg', 'h'],\n ['h/F09_phrases08_08.jpg', 'h'],\n ['h/F09_phrases08_09.jpg', 'h'],\n ['h/F09_phrases08_10.jpg', 'h'],\n ['h/F10_phrases08_01.jpg', 'h'],\n ['h/F10_phrases08_02.jpg', 'h'],\n ['h/F10_phrases08_03.jpg', 'h'],\n ['h/F10_phrases08_04.jpg', 'h'],\n ['h/F10_phrases08_05.jpg', 'h'],\n ['h/F10_phrases08_06.jpg', 'h'],\n ['h/F10_phrases08_07.jpg', 'h'],\n ['h/F10_phrases08_08.jpg', 'h'],\n ['h/F10_phrases08_09.jpg', 'h'],\n ['h/F10_phrases08_10.jpg', 'h'],\n ['h/F11_phrases08_01.jpg', 'h'],\n ['h/F11_phrases08_02.jpg', 'h'],\n ['h/F11_phrases08_03.jpg', 'h'],\n ['h/F11_phrases08_04.jpg', 'h'],\n ['h/F11_phrases08_05.jpg', 'h'],\n ['h/F11_phrases08_06.jpg', 'h'],\n ['h/F11_phrases08_07.jpg', 'h'],\n ['h/F11_phrases08_08.jpg', 'h'],\n ['h/F11_phrases08_09.jpg', 'h'],\n ['h/F11_phrases08_10.jpg', 'h'],\n ['h/M01_phrases08_01.jpg', 'h'],\n ['h/M01_phrases08_02.jpg', 'h'],\n ['h/M01_phrases08_03.jpg', 'h'],\n ['h/M01_phrases08_04.jpg', 'h'],\n ['h/M01_phrases08_05.jpg', 'h'],\n ['h/M01_phrases08_06.jpg', 'h'],\n ['h/M01_phrases08_07.jpg', 'h'],\n ['h/M01_phrases08_08.jpg', 'h'],\n ['h/M01_phrases08_09.jpg', 'h'],\n ['h/M01_phrases08_10.jpg', 'h'],\n ['h/M02_phrases08_01.jpg', 'h'],\n ['h/M02_phrases08_02.jpg', 'h'],\n ['h/M02_phrases08_03.jpg', 'h'],\n ['h/M02_phrases08_04.jpg', 'h'],\n ['h/M02_phrases08_05.jpg', 'h'],\n ['h/M02_phrases08_06.jpg', 'h'],\n ['h/M02_phrases08_07.jpg', 'h'],\n ['h/M02_phrases08_08.jpg', 'h'],\n ['h/M02_phrases08_09.jpg', 'h'],\n ['h/M02_phrases08_10.jpg', 'h'],\n ['h/M04_phrases08_01.jpg', 'h'],\n ['h/M04_phrases08_02.jpg', 'h'],\n ['h/M04_phrases08_03.jpg', 'h'],\n ['h/M04_phrases08_04.jpg', 'h'],\n ['h/M04_phrases08_05.jpg', 'h'],\n ...]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'C:\\\\Users\\\\Daniele\\\\PycharmProjects\\\\LipNetProve\\\\Dataset\\\\Train'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import csv\n",
    "os.chdir('../../codes')\n",
    "header = ['filename', 'label']\n",
    "\n",
    "with open('training_labels.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('training_labels.csv')\n",
    "Y = train_data[['label']]\n",
    "#80/20\n",
    "kf = KFold(n_splits = 5)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 7, shuffle = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "idg = ImageDataGenerator(rescale=1./255)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def create_new_model():\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from keras.models import Sequential # To initialise the nn as a sequence of layers\n",
    "    from keras.layers import Convolution2D # To make the convolution layer for 2D images\n",
    "    from keras.layers import MaxPooling2D #\n",
    "    from keras.layers import Flatten\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Dropout\n",
    "    from keras.callbacks import CSVLogger\n",
    "    from tensorflow.keras.optimizers import RMSprop\n",
    "    from keras.layers import BatchNormalization\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from keras.models import load_model\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.layers import Activation\n",
    "    from keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "\n",
    "    csv = CSVLogger(\"2_adam.log\")\n",
    "    #filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "    #checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "\n",
    "    # Initialising the CNN\n",
    "    classifier = Sequential()\n",
    "\n",
    "    # Step 1 - Convolution\n",
    "    classifier.add(Convolution2D(32, (2, 2), input_shape=(224, 224, 1), activation='relu', strides=2, name='convo1'))\n",
    "    classifier.add(Convolution2D(64, (3, 3), activation='relu', name='convo2'))\n",
    "    # Step 1 - Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Step 2 - Convolution\n",
    "    classifier.add(Convolution2D(64, (3, 3), activation='relu', name='convo3'))\n",
    "    # Step 2 - Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # Step 3 - Convolution\n",
    "    classifier.add(Convolution2D(64, (3, 3), activation='relu', name='convo4'))\n",
    "    # Step 3 - Pooling\n",
    "    classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #Step 4 - Flattening\n",
    "\n",
    "    classifier.add(Flatten())\n",
    "\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout((0.5)))\n",
    "    classifier.add(Dense(1024, activation='relu'))\n",
    "\n",
    "    '''classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout((0.5)))\n",
    "    classifier.add(Dense(512, activation = 'relu'))\n",
    "    '''\n",
    "\n",
    "    classifier.add(BatchNormalization())\n",
    "    classifier.add(Dropout((0.4)))\n",
    "    classifier.add(Dense(20, activation='softmax'))\n",
    "\n",
    "    return classifier\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2068 validated image filenames belonging to 20 classes.\n",
      "Found 517 validated image filenames belonging to 20 classes.\n",
      "Epoch 1/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 3.3666 - accuracy: 0.1480\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.05803, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 33s 505ms/step - loss: 3.3666 - accuracy: 0.1480 - val_loss: 2.9547 - val_accuracy: 0.0580\n",
      "Epoch 2/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 2.1815 - accuracy: 0.3839\n",
      "Epoch 00002: val_accuracy improved from 0.05803 to 0.06383, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 32s 487ms/step - loss: 2.1815 - accuracy: 0.3839 - val_loss: 2.9619 - val_accuracy: 0.0638\n",
      "Epoch 3/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.4325 - accuracy: 0.5580\n",
      "Epoch 00003: val_accuracy improved from 0.06383 to 0.08124, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 32s 489ms/step - loss: 1.4325 - accuracy: 0.5580 - val_loss: 2.8752 - val_accuracy: 0.0812\n",
      "Epoch 4/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.9396 - accuracy: 0.7036\n",
      "Epoch 00004: val_accuracy improved from 0.08124 to 0.14894, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 32s 491ms/step - loss: 0.9396 - accuracy: 0.7036 - val_loss: 2.7415 - val_accuracy: 0.1489\n",
      "Epoch 5/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.7785\n",
      "Epoch 00005: val_accuracy improved from 0.14894 to 0.20696, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 32s 496ms/step - loss: 0.6789 - accuracy: 0.7785 - val_loss: 2.4770 - val_accuracy: 0.2070\n",
      "Epoch 6/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5153 - accuracy: 0.8245\n",
      "Epoch 00006: val_accuracy improved from 0.20696 to 0.39458, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 33s 501ms/step - loss: 0.5153 - accuracy: 0.8245 - val_loss: 2.1858 - val_accuracy: 0.3946\n",
      "Epoch 7/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8917\n",
      "Epoch 00007: val_accuracy did not improve from 0.39458\n",
      "65/65 [==============================] - 32s 499ms/step - loss: 0.3476 - accuracy: 0.8917 - val_loss: 2.2006 - val_accuracy: 0.2979\n",
      "Epoch 8/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9081\n",
      "Epoch 00008: val_accuracy improved from 0.39458 to 0.47969, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 0.2723 - accuracy: 0.9081 - val_loss: 1.7542 - val_accuracy: 0.4797\n",
      "Epoch 9/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2479 - accuracy: 0.9236\n",
      "Epoch 00009: val_accuracy improved from 0.47969 to 0.53965, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 0.2479 - accuracy: 0.9236 - val_loss: 1.6484 - val_accuracy: 0.5397\n",
      "Epoch 10/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1908 - accuracy: 0.9420\n",
      "Epoch 00010: val_accuracy did not improve from 0.53965\n",
      "65/65 [==============================] - 30s 469ms/step - loss: 0.1908 - accuracy: 0.9420 - val_loss: 1.7136 - val_accuracy: 0.5068\n",
      "Epoch 11/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1415 - accuracy: 0.9541\n",
      "Epoch 00011: val_accuracy improved from 0.53965 to 0.58414, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 31s 475ms/step - loss: 0.1415 - accuracy: 0.9541 - val_loss: 1.4626 - val_accuracy: 0.5841\n",
      "Epoch 12/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9541\n",
      "Epoch 00012: val_accuracy improved from 0.58414 to 0.58607, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 0.1514 - accuracy: 0.9541 - val_loss: 1.5860 - val_accuracy: 0.5861\n",
      "Epoch 13/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9531\n",
      "Epoch 00013: val_accuracy did not improve from 0.58607\n",
      "65/65 [==============================] - 31s 470ms/step - loss: 0.1308 - accuracy: 0.9531 - val_loss: 1.7905 - val_accuracy: 0.5822\n",
      "Epoch 14/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1273 - accuracy: 0.9589\n",
      "Epoch 00014: val_accuracy improved from 0.58607 to 0.68279, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 31s 471ms/step - loss: 0.1273 - accuracy: 0.9589 - val_loss: 1.3395 - val_accuracy: 0.6828\n",
      "Epoch 15/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9744\n",
      "Epoch 00015: val_accuracy improved from 0.68279 to 0.68665, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 0.0971 - accuracy: 0.9744 - val_loss: 1.0668 - val_accuracy: 0.6867\n",
      "Epoch 16/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0973 - accuracy: 0.9720\n",
      "Epoch 00016: val_accuracy did not improve from 0.68665\n",
      "65/65 [==============================] - 31s 481ms/step - loss: 0.0973 - accuracy: 0.9720 - val_loss: 1.5658 - val_accuracy: 0.6673\n",
      "Epoch 17/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9739\n",
      "Epoch 00017: val_accuracy did not improve from 0.68665\n",
      "65/65 [==============================] - 31s 480ms/step - loss: 0.0862 - accuracy: 0.9739 - val_loss: 1.6703 - val_accuracy: 0.6228\n",
      "Epoch 18/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9753\n",
      "Epoch 00018: val_accuracy did not improve from 0.68665\n",
      "65/65 [==============================] - 32s 485ms/step - loss: 0.0835 - accuracy: 0.9753 - val_loss: 2.0996 - val_accuracy: 0.5706\n",
      "Epoch 19/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9695\n",
      "Epoch 00019: val_accuracy improved from 0.68665 to 0.74275, saving model to saved_models\\model_1.h5\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.0828 - accuracy: 0.9695 - val_loss: 1.0087 - val_accuracy: 0.7427\n",
      "Epoch 20/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9792\n",
      "Epoch 00020: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 0.0733 - accuracy: 0.9792 - val_loss: 2.1682 - val_accuracy: 0.5706\n",
      "Epoch 21/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0804 - accuracy: 0.9720\n",
      "Epoch 00021: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 37s 563ms/step - loss: 0.0804 - accuracy: 0.9720 - val_loss: 1.8219 - val_accuracy: 0.6112\n",
      "Epoch 22/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9753\n",
      "Epoch 00022: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 36s 555ms/step - loss: 0.0784 - accuracy: 0.9753 - val_loss: 2.1743 - val_accuracy: 0.6190\n",
      "Epoch 23/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9758\n",
      "Epoch 00023: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 0.0739 - accuracy: 0.9758 - val_loss: 3.4464 - val_accuracy: 0.4642\n",
      "Epoch 24/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9715\n",
      "Epoch 00024: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 33s 509ms/step - loss: 0.0975 - accuracy: 0.9715 - val_loss: 1.2456 - val_accuracy: 0.7215\n",
      "Epoch 25/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.9763\n",
      "Epoch 00025: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 33s 510ms/step - loss: 0.0848 - accuracy: 0.9763 - val_loss: 1.6125 - val_accuracy: 0.6499\n",
      "Epoch 26/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9720\n",
      "Epoch 00026: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 33s 508ms/step - loss: 0.0855 - accuracy: 0.9720 - val_loss: 2.9293 - val_accuracy: 0.5222\n",
      "Epoch 27/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9734\n",
      "Epoch 00027: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 34s 527ms/step - loss: 0.0821 - accuracy: 0.9734 - val_loss: 2.4811 - val_accuracy: 0.5571\n",
      "Epoch 28/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9628\n",
      "Epoch 00028: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 34s 525ms/step - loss: 0.1102 - accuracy: 0.9628 - val_loss: 2.8758 - val_accuracy: 0.5571\n",
      "Epoch 29/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9749\n",
      "Epoch 00029: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 33s 511ms/step - loss: 0.0837 - accuracy: 0.9749 - val_loss: 5.0636 - val_accuracy: 0.3946\n",
      "Epoch 30/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0784 - accuracy: 0.9739\n",
      "Epoch 00030: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 36s 550ms/step - loss: 0.0784 - accuracy: 0.9739 - val_loss: 1.5290 - val_accuracy: 0.6809\n",
      "Epoch 31/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9715\n",
      "Epoch 00031: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 39s 600ms/step - loss: 0.0928 - accuracy: 0.9715 - val_loss: 3.2095 - val_accuracy: 0.4952\n",
      "Epoch 32/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9695\n",
      "Epoch 00032: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 38s 584ms/step - loss: 0.0911 - accuracy: 0.9695 - val_loss: 2.2946 - val_accuracy: 0.5977\n",
      "Epoch 33/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9705\n",
      "Epoch 00033: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 36s 552ms/step - loss: 0.0878 - accuracy: 0.9705 - val_loss: 2.9678 - val_accuracy: 0.5048\n",
      "Epoch 34/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9787\n",
      "Epoch 00034: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.0699 - accuracy: 0.9787 - val_loss: 2.2674 - val_accuracy: 0.5919\n",
      "Epoch 35/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9840\n",
      "Epoch 00035: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 37s 568ms/step - loss: 0.0544 - accuracy: 0.9840 - val_loss: 1.7587 - val_accuracy: 0.6325\n",
      "Epoch 36/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9845\n",
      "Epoch 00036: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 0.0563 - accuracy: 0.9845 - val_loss: 5.0839 - val_accuracy: 0.4120\n",
      "Epoch 37/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9778\n",
      "Epoch 00037: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 36s 551ms/step - loss: 0.0691 - accuracy: 0.9778 - val_loss: 6.4627 - val_accuracy: 0.3520\n",
      "Epoch 38/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9753\n",
      "Epoch 00038: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 37s 565ms/step - loss: 0.0728 - accuracy: 0.9753 - val_loss: 4.6725 - val_accuracy: 0.4371\n",
      "Epoch 39/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9845\n",
      "Epoch 00039: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 36s 557ms/step - loss: 0.0553 - accuracy: 0.9845 - val_loss: 1.9895 - val_accuracy: 0.6074\n",
      "Epoch 40/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9729\n",
      "Epoch 00040: val_accuracy did not improve from 0.74275\n",
      "65/65 [==============================] - 36s 552ms/step - loss: 0.0773 - accuracy: 0.9729 - val_loss: 1.5153 - val_accuracy: 0.7099\n",
      "17/17 [==============================] - 1s 78ms/step - loss: 1.0087 - accuracy: 0.7427\n",
      "Found 2068 validated image filenames belonging to 20 classes.\n",
      "Found 517 validated image filenames belonging to 20 classes.\n",
      "Epoch 1/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 3.2825 - accuracy: 0.1799\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.17988, saving model to saved_models\\model_2.h5\n",
      "65/65 [==============================] - 36s 546ms/step - loss: 3.2825 - accuracy: 0.1799 - val_loss: 2.9145 - val_accuracy: 0.1799\n",
      "Epoch 2/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 2.0827 - accuracy: 0.3931\n",
      "Epoch 00002: val_accuracy did not improve from 0.17988\n",
      "65/65 [==============================] - 36s 554ms/step - loss: 2.0827 - accuracy: 0.3931 - val_loss: 2.8708 - val_accuracy: 0.1044\n",
      "Epoch 3/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.5806 - accuracy: 0.5150\n",
      "Epoch 00003: val_accuracy improved from 0.17988 to 0.18375, saving model to saved_models\\model_2.h5\n",
      "65/65 [==============================] - 36s 552ms/step - loss: 1.5806 - accuracy: 0.5150 - val_loss: 2.7507 - val_accuracy: 0.1838\n",
      "Epoch 4/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.1377 - accuracy: 0.6489\n",
      "Epoch 00004: val_accuracy improved from 0.18375 to 0.24178, saving model to saved_models\\model_2.h5\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 1.1377 - accuracy: 0.6489 - val_loss: 2.6331 - val_accuracy: 0.2418\n",
      "Epoch 5/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.7933 - accuracy: 0.7471\n",
      "Epoch 00005: val_accuracy did not improve from 0.24178\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 0.7933 - accuracy: 0.7471 - val_loss: 2.7727 - val_accuracy: 0.1818\n",
      "Epoch 6/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.7955\n",
      "Epoch 00006: val_accuracy improved from 0.24178 to 0.35977, saving model to saved_models\\model_2.h5\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 0.6226 - accuracy: 0.7955 - val_loss: 2.0955 - val_accuracy: 0.3598\n",
      "Epoch 7/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4461 - accuracy: 0.8506\n",
      "Epoch 00007: val_accuracy improved from 0.35977 to 0.56867, saving model to saved_models\\model_2.h5\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 0.4461 - accuracy: 0.8506 - val_loss: 1.3944 - val_accuracy: 0.5687\n",
      "Epoch 8/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.8868\n",
      "Epoch 00008: val_accuracy did not improve from 0.56867\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 0.3515 - accuracy: 0.8868 - val_loss: 1.3478 - val_accuracy: 0.5667\n",
      "Epoch 9/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2519 - accuracy: 0.9144\n",
      "Epoch 00009: val_accuracy did not improve from 0.56867\n",
      "65/65 [==============================] - 34s 525ms/step - loss: 0.2519 - accuracy: 0.9144 - val_loss: 1.4883 - val_accuracy: 0.5164\n",
      "Epoch 10/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9265\n",
      "Epoch 00010: val_accuracy improved from 0.56867 to 0.75435, saving model to saved_models\\model_2.h5\n",
      "65/65 [==============================] - 34s 531ms/step - loss: 0.2221 - accuracy: 0.9265 - val_loss: 0.8527 - val_accuracy: 0.7544\n",
      "Epoch 11/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2096 - accuracy: 0.9304\n",
      "Epoch 00011: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 531ms/step - loss: 0.2096 - accuracy: 0.9304 - val_loss: 1.3272 - val_accuracy: 0.6190\n",
      "Epoch 12/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1941 - accuracy: 0.9425\n",
      "Epoch 00012: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 0.1941 - accuracy: 0.9425 - val_loss: 0.9753 - val_accuracy: 0.6963\n",
      "Epoch 13/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9425\n",
      "Epoch 00013: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 0.1769 - accuracy: 0.9425 - val_loss: 1.7516 - val_accuracy: 0.5996\n",
      "Epoch 14/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9492\n",
      "Epoch 00014: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 37s 563ms/step - loss: 0.1553 - accuracy: 0.9492 - val_loss: 1.1514 - val_accuracy: 0.7002\n",
      "Epoch 15/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9541\n",
      "Epoch 00015: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 37s 570ms/step - loss: 0.1355 - accuracy: 0.9541 - val_loss: 1.1639 - val_accuracy: 0.6809\n",
      "Epoch 16/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9579\n",
      "Epoch 00016: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 36s 553ms/step - loss: 0.1187 - accuracy: 0.9579 - val_loss: 1.1376 - val_accuracy: 0.7157\n",
      "Epoch 17/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1191 - accuracy: 0.9637\n",
      "Epoch 00017: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 33s 512ms/step - loss: 0.1191 - accuracy: 0.9637 - val_loss: 0.9265 - val_accuracy: 0.7350\n",
      "Epoch 18/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9753\n",
      "Epoch 00018: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 525ms/step - loss: 0.0952 - accuracy: 0.9753 - val_loss: 1.6118 - val_accuracy: 0.6074\n",
      "Epoch 19/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9681\n",
      "Epoch 00019: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 0.0989 - accuracy: 0.9681 - val_loss: 1.2742 - val_accuracy: 0.6905\n",
      "Epoch 20/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9807\n",
      "Epoch 00020: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 518ms/step - loss: 0.0738 - accuracy: 0.9807 - val_loss: 1.2091 - val_accuracy: 0.7137\n",
      "Epoch 21/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9782\n",
      "Epoch 00021: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 0.0717 - accuracy: 0.9782 - val_loss: 1.5660 - val_accuracy: 0.6306\n",
      "Epoch 22/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9749\n",
      "Epoch 00022: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 531ms/step - loss: 0.0865 - accuracy: 0.9749 - val_loss: 2.2846 - val_accuracy: 0.5706\n",
      "Epoch 23/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9797\n",
      "Epoch 00023: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 526ms/step - loss: 0.0632 - accuracy: 0.9797 - val_loss: 1.7978 - val_accuracy: 0.6557\n",
      "Epoch 24/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9821\n",
      "Epoch 00024: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 38s 583ms/step - loss: 0.0607 - accuracy: 0.9821 - val_loss: 1.4477 - val_accuracy: 0.6538\n",
      "Epoch 25/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9816\n",
      "Epoch 00025: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 36s 546ms/step - loss: 0.0677 - accuracy: 0.9816 - val_loss: 2.0034 - val_accuracy: 0.6035\n",
      "Epoch 26/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9787\n",
      "Epoch 00026: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 0.0758 - accuracy: 0.9787 - val_loss: 1.0321 - val_accuracy: 0.7447\n",
      "Epoch 27/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9797\n",
      "Epoch 00027: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 0.0691 - accuracy: 0.9797 - val_loss: 3.1891 - val_accuracy: 0.5126\n",
      "Epoch 28/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9715\n",
      "Epoch 00028: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.0858 - accuracy: 0.9715 - val_loss: 1.1793 - val_accuracy: 0.7099\n",
      "Epoch 29/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9720\n",
      "Epoch 00029: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.0860 - accuracy: 0.9720 - val_loss: 1.9185 - val_accuracy: 0.6518\n",
      "Epoch 30/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9811\n",
      "Epoch 00030: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 0.0672 - accuracy: 0.9811 - val_loss: 1.7528 - val_accuracy: 0.6692\n",
      "Epoch 31/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9826\n",
      "Epoch 00031: val_accuracy did not improve from 0.75435\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.0616 - accuracy: 0.9826 - val_loss: 2.1926 - val_accuracy: 0.5745\n",
      "Epoch 32/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9845\n",
      "Epoch 00032: val_accuracy improved from 0.75435 to 0.79497, saving model to saved_models\\model_2.h5\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.0589 - accuracy: 0.9845 - val_loss: 0.8243 - val_accuracy: 0.7950\n",
      "Epoch 33/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9797\n",
      "Epoch 00033: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 0.0589 - accuracy: 0.9797 - val_loss: 0.9774 - val_accuracy: 0.7737\n",
      "Epoch 34/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9845\n",
      "Epoch 00034: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.0475 - accuracy: 0.9845 - val_loss: 1.3020 - val_accuracy: 0.7118\n",
      "Epoch 35/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9826\n",
      "Epoch 00035: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 0.0546 - accuracy: 0.9826 - val_loss: 1.1843 - val_accuracy: 0.7060\n",
      "Epoch 36/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9836\n",
      "Epoch 00036: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.0519 - accuracy: 0.9836 - val_loss: 1.2383 - val_accuracy: 0.7234\n",
      "Epoch 37/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9816\n",
      "Epoch 00037: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 0.0566 - accuracy: 0.9816 - val_loss: 1.2111 - val_accuracy: 0.7427\n",
      "Epoch 38/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9821\n",
      "Epoch 00038: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.0652 - accuracy: 0.9821 - val_loss: 4.7963 - val_accuracy: 0.4023\n",
      "Epoch 39/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9836\n",
      "Epoch 00039: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 0.0544 - accuracy: 0.9836 - val_loss: 0.9057 - val_accuracy: 0.7930\n",
      "Epoch 40/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9869\n",
      "Epoch 00040: val_accuracy did not improve from 0.79497\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 0.0475 - accuracy: 0.9869 - val_loss: 1.8025 - val_accuracy: 0.6074\n",
      "17/17 [==============================] - 1s 79ms/step - loss: 0.8243 - accuracy: 0.7950\n",
      "Found 2068 validated image filenames belonging to 20 classes.\n",
      "Found 517 validated image filenames belonging to 20 classes.\n",
      "Epoch 1/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 3.2422 - accuracy: 0.1871\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.08897, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 36s 540ms/step - loss: 3.2422 - accuracy: 0.1871 - val_loss: 2.9628 - val_accuracy: 0.0890\n",
      "Epoch 2/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 2.0320 - accuracy: 0.4221\n",
      "Epoch 00002: val_accuracy improved from 0.08897 to 0.12379, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 539ms/step - loss: 2.0320 - accuracy: 0.4221 - val_loss: 2.9465 - val_accuracy: 0.1238\n",
      "Epoch 3/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.2723 - accuracy: 0.6194\n",
      "Epoch 00003: val_accuracy improved from 0.12379 to 0.20503, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 1.2723 - accuracy: 0.6194 - val_loss: 2.8972 - val_accuracy: 0.2050\n",
      "Epoch 4/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.8553 - accuracy: 0.7326\n",
      "Epoch 00004: val_accuracy improved from 0.20503 to 0.23985, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 546ms/step - loss: 0.8553 - accuracy: 0.7326 - val_loss: 2.8207 - val_accuracy: 0.2398\n",
      "Epoch 5/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6149 - accuracy: 0.8090\n",
      "Epoch 00005: val_accuracy did not improve from 0.23985\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.6149 - accuracy: 0.8090 - val_loss: 2.7262 - val_accuracy: 0.1044\n",
      "Epoch 6/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4422 - accuracy: 0.8515\n",
      "Epoch 00006: val_accuracy improved from 0.23985 to 0.30368, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.4422 - accuracy: 0.8515 - val_loss: 2.3058 - val_accuracy: 0.3037\n",
      "Epoch 7/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3088 - accuracy: 0.8941\n",
      "Epoch 00007: val_accuracy did not improve from 0.30368\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.3088 - accuracy: 0.8941 - val_loss: 3.8928 - val_accuracy: 0.0967\n",
      "Epoch 8/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3157 - accuracy: 0.8965\n",
      "Epoch 00008: val_accuracy did not improve from 0.30368\n",
      "65/65 [==============================] - 36s 546ms/step - loss: 0.3157 - accuracy: 0.8965 - val_loss: 2.5585 - val_accuracy: 0.3037\n",
      "Epoch 9/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2221 - accuracy: 0.9309\n",
      "Epoch 00009: val_accuracy improved from 0.30368 to 0.37911, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 36s 549ms/step - loss: 0.2221 - accuracy: 0.9309 - val_loss: 2.1434 - val_accuracy: 0.3791\n",
      "Epoch 10/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9347\n",
      "Epoch 00010: val_accuracy did not improve from 0.37911\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.1912 - accuracy: 0.9347 - val_loss: 3.4817 - val_accuracy: 0.2979\n",
      "Epoch 11/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9487\n",
      "Epoch 00011: val_accuracy improved from 0.37911 to 0.56673, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 36s 547ms/step - loss: 0.1606 - accuracy: 0.9487 - val_loss: 1.7371 - val_accuracy: 0.5667\n",
      "Epoch 12/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9550\n",
      "Epoch 00012: val_accuracy did not improve from 0.56673\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.1287 - accuracy: 0.9550 - val_loss: 1.8846 - val_accuracy: 0.5164\n",
      "Epoch 13/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9550\n",
      "Epoch 00013: val_accuracy improved from 0.56673 to 0.63830, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 0.1436 - accuracy: 0.9550 - val_loss: 1.4160 - val_accuracy: 0.6383\n",
      "Epoch 14/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9652\n",
      "Epoch 00014: val_accuracy did not improve from 0.63830\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 0.1114 - accuracy: 0.9652 - val_loss: 2.4256 - val_accuracy: 0.5513\n",
      "Epoch 15/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9811\n",
      "Epoch 00015: val_accuracy did not improve from 0.63830\n",
      "65/65 [==============================] - 36s 546ms/step - loss: 0.0673 - accuracy: 0.9811 - val_loss: 1.7315 - val_accuracy: 0.5996\n",
      "Epoch 16/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0910 - accuracy: 0.9753\n",
      "Epoch 00016: val_accuracy did not improve from 0.63830\n",
      "65/65 [==============================] - 35s 546ms/step - loss: 0.0910 - accuracy: 0.9753 - val_loss: 1.5914 - val_accuracy: 0.6383\n",
      "Epoch 17/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9778\n",
      "Epoch 00017: val_accuracy improved from 0.63830 to 0.67118, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 541ms/step - loss: 0.0750 - accuracy: 0.9778 - val_loss: 1.6344 - val_accuracy: 0.6712\n",
      "Epoch 18/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9855\n",
      "Epoch 00018: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 36s 547ms/step - loss: 0.0508 - accuracy: 0.9855 - val_loss: 2.5483 - val_accuracy: 0.5416\n",
      "Epoch 19/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9898\n",
      "Epoch 00019: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 546ms/step - loss: 0.0396 - accuracy: 0.9898 - val_loss: 1.3929 - val_accuracy: 0.6557\n",
      "Epoch 20/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9855\n",
      "Epoch 00020: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 0.0530 - accuracy: 0.9855 - val_loss: 1.4978 - val_accuracy: 0.6248\n",
      "Epoch 21/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9797\n",
      "Epoch 00021: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 36s 549ms/step - loss: 0.0680 - accuracy: 0.9797 - val_loss: 3.3196 - val_accuracy: 0.4313\n",
      "Epoch 22/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9773\n",
      "Epoch 00022: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.0761 - accuracy: 0.9773 - val_loss: 4.1900 - val_accuracy: 0.4526\n",
      "Epoch 23/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9671\n",
      "Epoch 00023: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 0.0993 - accuracy: 0.9671 - val_loss: 2.7321 - val_accuracy: 0.5377\n",
      "Epoch 24/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9739\n",
      "Epoch 00024: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 0.0850 - accuracy: 0.9739 - val_loss: 2.4696 - val_accuracy: 0.5416\n",
      "Epoch 25/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9797\n",
      "Epoch 00025: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 0.0710 - accuracy: 0.9797 - val_loss: 4.6437 - val_accuracy: 0.4603\n",
      "Epoch 26/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9845\n",
      "Epoch 00026: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 0.0537 - accuracy: 0.9845 - val_loss: 5.9686 - val_accuracy: 0.3172\n",
      "Epoch 27/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9705\n",
      "Epoch 00027: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 0.0887 - accuracy: 0.9705 - val_loss: 24.3091 - val_accuracy: 0.1199\n",
      "Epoch 28/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9729\n",
      "Epoch 00028: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 36s 550ms/step - loss: 0.0766 - accuracy: 0.9729 - val_loss: 13.3873 - val_accuracy: 0.1663\n",
      "Epoch 29/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9637\n",
      "Epoch 00029: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 36s 548ms/step - loss: 0.0978 - accuracy: 0.9637 - val_loss: 6.6532 - val_accuracy: 0.3617\n",
      "Epoch 30/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0793 - accuracy: 0.9734\n",
      "Epoch 00030: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 0.0793 - accuracy: 0.9734 - val_loss: 2.2856 - val_accuracy: 0.5745\n",
      "Epoch 31/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9860\n",
      "Epoch 00031: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 36s 547ms/step - loss: 0.0525 - accuracy: 0.9860 - val_loss: 5.5267 - val_accuracy: 0.3830\n",
      "Epoch 32/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9816\n",
      "Epoch 00032: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 36s 546ms/step - loss: 0.0568 - accuracy: 0.9816 - val_loss: 2.9689 - val_accuracy: 0.5087\n",
      "Epoch 33/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9826\n",
      "Epoch 00033: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 35s 546ms/step - loss: 0.0542 - accuracy: 0.9826 - val_loss: 5.2570 - val_accuracy: 0.3037\n",
      "Epoch 34/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9826\n",
      "Epoch 00034: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 34s 519ms/step - loss: 0.0570 - accuracy: 0.9826 - val_loss: 2.1539 - val_accuracy: 0.5629\n",
      "Epoch 35/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9811\n",
      "Epoch 00035: val_accuracy did not improve from 0.67118\n",
      "65/65 [==============================] - 34s 530ms/step - loss: 0.0576 - accuracy: 0.9811 - val_loss: 2.9433 - val_accuracy: 0.5242\n",
      "Epoch 36/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9840\n",
      "Epoch 00036: val_accuracy improved from 0.67118 to 0.67505, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.0552 - accuracy: 0.9840 - val_loss: 1.5402 - val_accuracy: 0.6750\n",
      "Epoch 37/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9860\n",
      "Epoch 00037: val_accuracy did not improve from 0.67505\n",
      "65/65 [==============================] - 34s 524ms/step - loss: 0.0440 - accuracy: 0.9860 - val_loss: 3.5849 - val_accuracy: 0.4197\n",
      "Epoch 38/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9802\n",
      "Epoch 00038: val_accuracy did not improve from 0.67505\n",
      "65/65 [==============================] - 36s 554ms/step - loss: 0.0618 - accuracy: 0.9802 - val_loss: 4.7094 - val_accuracy: 0.4101\n",
      "Epoch 39/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9821\n",
      "Epoch 00039: val_accuracy improved from 0.67505 to 0.73308, saving model to saved_models\\model_3.h5\n",
      "65/65 [==============================] - 35s 546ms/step - loss: 0.0541 - accuracy: 0.9821 - val_loss: 1.2493 - val_accuracy: 0.7331\n",
      "Epoch 40/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9840\n",
      "Epoch 00040: val_accuracy did not improve from 0.73308\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.0532 - accuracy: 0.9840 - val_loss: 2.5241 - val_accuracy: 0.5861\n",
      "17/17 [==============================] - 1s 76ms/step - loss: 1.2493 - accuracy: 0.7331\n",
      "Found 2068 validated image filenames belonging to 20 classes.\n",
      "Found 517 validated image filenames belonging to 20 classes.\n",
      "Epoch 1/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 3.3518 - accuracy: 0.1601\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.11799, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 36s 541ms/step - loss: 3.3518 - accuracy: 0.1601 - val_loss: 2.9605 - val_accuracy: 0.1180\n",
      "Epoch 2/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 2.1660 - accuracy: 0.3801\n",
      "Epoch 00002: val_accuracy did not improve from 0.11799\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 2.1660 - accuracy: 0.3801 - val_loss: 2.9667 - val_accuracy: 0.1044\n",
      "Epoch 3/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.5396 - accuracy: 0.5455\n",
      "Epoch 00003: val_accuracy did not improve from 0.11799\n",
      "65/65 [==============================] - 34s 529ms/step - loss: 1.5396 - accuracy: 0.5455 - val_loss: 2.8865 - val_accuracy: 0.0851\n",
      "Epoch 4/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.1560 - accuracy: 0.6301\n",
      "Epoch 00004: val_accuracy did not improve from 0.11799\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 1.1560 - accuracy: 0.6301 - val_loss: 2.9853 - val_accuracy: 0.0658\n",
      "Epoch 5/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.7680 - accuracy: 0.7481\n",
      "Epoch 00005: val_accuracy improved from 0.11799 to 0.20890, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 0.7680 - accuracy: 0.7481 - val_loss: 2.7917 - val_accuracy: 0.2089\n",
      "Epoch 6/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.6038 - accuracy: 0.7959\n",
      "Epoch 00006: val_accuracy improved from 0.20890 to 0.31335, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 0.6038 - accuracy: 0.7959 - val_loss: 2.1951 - val_accuracy: 0.3133\n",
      "Epoch 7/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.4370 - accuracy: 0.8603\n",
      "Epoch 00007: val_accuracy improved from 0.31335 to 0.36364, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.4370 - accuracy: 0.8603 - val_loss: 1.9446 - val_accuracy: 0.3636\n",
      "Epoch 8/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.8902\n",
      "Epoch 00008: val_accuracy improved from 0.36364 to 0.53965, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.3316 - accuracy: 0.8902 - val_loss: 1.3893 - val_accuracy: 0.5397\n",
      "Epoch 9/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2656 - accuracy: 0.9125\n",
      "Epoch 00009: val_accuracy improved from 0.53965 to 0.60735, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 0.2656 - accuracy: 0.9125 - val_loss: 1.1519 - val_accuracy: 0.6074\n",
      "Epoch 10/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9241\n",
      "Epoch 00010: val_accuracy improved from 0.60735 to 0.69246, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.2435 - accuracy: 0.9241 - val_loss: 0.9996 - val_accuracy: 0.6925\n",
      "Epoch 11/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2177 - accuracy: 0.9241\n",
      "Epoch 00011: val_accuracy did not improve from 0.69246\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 0.2177 - accuracy: 0.9241 - val_loss: 2.0500 - val_accuracy: 0.5300\n",
      "Epoch 12/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1863 - accuracy: 0.9454\n",
      "Epoch 00012: val_accuracy did not improve from 0.69246\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.1863 - accuracy: 0.9454 - val_loss: 1.6170 - val_accuracy: 0.5938\n",
      "Epoch 13/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1630 - accuracy: 0.9497\n",
      "Epoch 00013: val_accuracy did not improve from 0.69246\n",
      "65/65 [==============================] - 35s 531ms/step - loss: 0.1630 - accuracy: 0.9497 - val_loss: 1.4550 - val_accuracy: 0.5996\n",
      "Epoch 14/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1419 - accuracy: 0.9574\n",
      "Epoch 00014: val_accuracy did not improve from 0.69246\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.1419 - accuracy: 0.9574 - val_loss: 2.2220 - val_accuracy: 0.5435\n",
      "Epoch 15/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1601 - accuracy: 0.9444\n",
      "Epoch 00015: val_accuracy improved from 0.69246 to 0.73114, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.1601 - accuracy: 0.9444 - val_loss: 1.1524 - val_accuracy: 0.7311\n",
      "Epoch 16/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1072 - accuracy: 0.9691\n",
      "Epoch 00016: val_accuracy did not improve from 0.73114\n",
      "65/65 [==============================] - 35s 532ms/step - loss: 0.1072 - accuracy: 0.9691 - val_loss: 1.1040 - val_accuracy: 0.7311\n",
      "Epoch 17/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1155 - accuracy: 0.9623\n",
      "Epoch 00017: val_accuracy did not improve from 0.73114\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.1155 - accuracy: 0.9623 - val_loss: 1.3551 - val_accuracy: 0.6306\n",
      "Epoch 18/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9565\n",
      "Epoch 00018: val_accuracy did not improve from 0.73114\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.1340 - accuracy: 0.9565 - val_loss: 2.6594 - val_accuracy: 0.5532\n",
      "Epoch 19/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0989 - accuracy: 0.9700\n",
      "Epoch 00019: val_accuracy did not improve from 0.73114\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 0.0989 - accuracy: 0.9700 - val_loss: 2.1175 - val_accuracy: 0.5280\n",
      "Epoch 20/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1176 - accuracy: 0.9608\n",
      "Epoch 00020: val_accuracy did not improve from 0.73114\n",
      "65/65 [==============================] - 35s 536ms/step - loss: 0.1176 - accuracy: 0.9608 - val_loss: 1.7810 - val_accuracy: 0.6074\n",
      "Epoch 21/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9739\n",
      "Epoch 00021: val_accuracy did not improve from 0.73114\n",
      "65/65 [==============================] - 35s 543ms/step - loss: 0.0958 - accuracy: 0.9739 - val_loss: 1.1702 - val_accuracy: 0.7195\n",
      "Epoch 22/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9787\n",
      "Epoch 00022: val_accuracy improved from 0.73114 to 0.78143, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.0750 - accuracy: 0.9787 - val_loss: 0.8084 - val_accuracy: 0.7814\n",
      "Epoch 23/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9744\n",
      "Epoch 00023: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.0739 - accuracy: 0.9744 - val_loss: 1.5122 - val_accuracy: 0.6925\n",
      "Epoch 24/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0616 - accuracy: 0.9826\n",
      "Epoch 00024: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.0616 - accuracy: 0.9826 - val_loss: 1.3014 - val_accuracy: 0.7273\n",
      "Epoch 25/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9705\n",
      "Epoch 00025: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 0.0825 - accuracy: 0.9705 - val_loss: 1.8588 - val_accuracy: 0.6267\n",
      "Epoch 26/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0723 - accuracy: 0.9778\n",
      "Epoch 00026: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 35s 535ms/step - loss: 0.0723 - accuracy: 0.9778 - val_loss: 2.3875 - val_accuracy: 0.5435\n",
      "Epoch 27/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9720\n",
      "Epoch 00027: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.0809 - accuracy: 0.9720 - val_loss: 5.5672 - val_accuracy: 0.3075\n",
      "Epoch 28/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9749\n",
      "Epoch 00028: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 35s 533ms/step - loss: 0.0821 - accuracy: 0.9749 - val_loss: 2.4471 - val_accuracy: 0.5706\n",
      "Epoch 29/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1178 - accuracy: 0.9613\n",
      "Epoch 00029: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 35s 537ms/step - loss: 0.1178 - accuracy: 0.9613 - val_loss: 3.7996 - val_accuracy: 0.4662\n",
      "Epoch 30/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9705\n",
      "Epoch 00030: val_accuracy improved from 0.78143 to 0.79304, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.0907 - accuracy: 0.9705 - val_loss: 0.8587 - val_accuracy: 0.7930\n",
      "Epoch 31/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9773\n",
      "Epoch 00031: val_accuracy did not improve from 0.79304\n",
      "65/65 [==============================] - 35s 534ms/step - loss: 0.0706 - accuracy: 0.9773 - val_loss: 1.9082 - val_accuracy: 0.5783\n",
      "Epoch 32/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9826\n",
      "Epoch 00032: val_accuracy did not improve from 0.79304\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 0.0571 - accuracy: 0.9826 - val_loss: 2.6020 - val_accuracy: 0.5687\n",
      "Epoch 33/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9778\n",
      "Epoch 00033: val_accuracy did not improve from 0.79304\n",
      "65/65 [==============================] - 35s 538ms/step - loss: 0.0688 - accuracy: 0.9778 - val_loss: 2.9598 - val_accuracy: 0.5455\n",
      "Epoch 34/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9831\n",
      "Epoch 00034: val_accuracy did not improve from 0.79304\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 0.0590 - accuracy: 0.9831 - val_loss: 0.9703 - val_accuracy: 0.7679\n",
      "Epoch 35/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9816\n",
      "Epoch 00035: val_accuracy did not improve from 0.79304\n",
      "65/65 [==============================] - 35s 542ms/step - loss: 0.0656 - accuracy: 0.9816 - val_loss: 0.9556 - val_accuracy: 0.7776\n",
      "Epoch 36/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9836\n",
      "Epoch 00036: val_accuracy did not improve from 0.79304\n",
      "65/65 [==============================] - 35s 540ms/step - loss: 0.0558 - accuracy: 0.9836 - val_loss: 1.2485 - val_accuracy: 0.7118\n",
      "Epoch 37/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0578 - accuracy: 0.9807\n",
      "Epoch 00037: val_accuracy improved from 0.79304 to 0.81238, saving model to saved_models\\model_4.h5\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 0.0578 - accuracy: 0.9807 - val_loss: 0.6722 - val_accuracy: 0.8124\n",
      "Epoch 38/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9763\n",
      "Epoch 00038: val_accuracy did not improve from 0.81238\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.0695 - accuracy: 0.9763 - val_loss: 1.5356 - val_accuracy: 0.6692\n",
      "Epoch 39/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0558 - accuracy: 0.9816\n",
      "Epoch 00039: val_accuracy did not improve from 0.81238\n",
      "65/65 [==============================] - 35s 544ms/step - loss: 0.0558 - accuracy: 0.9816 - val_loss: 1.0060 - val_accuracy: 0.7485\n",
      "Epoch 40/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9763\n",
      "Epoch 00040: val_accuracy did not improve from 0.81238\n",
      "65/65 [==============================] - 35s 545ms/step - loss: 0.0713 - accuracy: 0.9763 - val_loss: 4.0168 - val_accuracy: 0.3985\n",
      "17/17 [==============================] - 1s 78ms/step - loss: 0.6722 - accuracy: 0.8124\n",
      "Found 2068 validated image filenames belonging to 20 classes.\n",
      "Found 517 validated image filenames belonging to 20 classes.\n",
      "Epoch 1/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 3.3404 - accuracy: 0.1605\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.07737, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 38s 578ms/step - loss: 3.3404 - accuracy: 0.1605 - val_loss: 2.9375 - val_accuracy: 0.0774\n",
      "Epoch 2/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 2.1466 - accuracy: 0.3975\n",
      "Epoch 00002: val_accuracy did not improve from 0.07737\n",
      "65/65 [==============================] - 37s 571ms/step - loss: 2.1466 - accuracy: 0.3975 - val_loss: 2.9801 - val_accuracy: 0.0735\n",
      "Epoch 3/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 1.4078 - accuracy: 0.5696\n",
      "Epoch 00003: val_accuracy improved from 0.07737 to 0.11992, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 574ms/step - loss: 1.4078 - accuracy: 0.5696 - val_loss: 2.8714 - val_accuracy: 0.1199\n",
      "Epoch 4/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.9723 - accuracy: 0.7007\n",
      "Epoch 00004: val_accuracy improved from 0.11992 to 0.24758, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 575ms/step - loss: 0.9723 - accuracy: 0.7007 - val_loss: 2.6865 - val_accuracy: 0.2476\n",
      "Epoch 5/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.7163 - accuracy: 0.7631\n",
      "Epoch 00005: val_accuracy improved from 0.24758 to 0.28627, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 575ms/step - loss: 0.7163 - accuracy: 0.7631 - val_loss: 2.4370 - val_accuracy: 0.2863\n",
      "Epoch 6/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.8298\n",
      "Epoch 00006: val_accuracy improved from 0.28627 to 0.38685, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 573ms/step - loss: 0.5422 - accuracy: 0.8298 - val_loss: 1.9453 - val_accuracy: 0.3868\n",
      "Epoch 7/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3753 - accuracy: 0.8791\n",
      "Epoch 00007: val_accuracy did not improve from 0.38685\n",
      "65/65 [==============================] - 37s 567ms/step - loss: 0.3753 - accuracy: 0.8791 - val_loss: 1.9598 - val_accuracy: 0.3849\n",
      "Epoch 8/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.3616 - accuracy: 0.8796\n",
      "Epoch 00008: val_accuracy improved from 0.38685 to 0.56673, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 579ms/step - loss: 0.3616 - accuracy: 0.8796 - val_loss: 1.3670 - val_accuracy: 0.5667\n",
      "Epoch 9/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9028\n",
      "Epoch 00009: val_accuracy improved from 0.56673 to 0.58027, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 575ms/step - loss: 0.2852 - accuracy: 0.9028 - val_loss: 1.3555 - val_accuracy: 0.5803\n",
      "Epoch 10/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.2140 - accuracy: 0.9338\n",
      "Epoch 00010: val_accuracy improved from 0.58027 to 0.59188, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 0.2140 - accuracy: 0.9338 - val_loss: 1.2523 - val_accuracy: 0.5919\n",
      "Epoch 11/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1737 - accuracy: 0.9449\n",
      "Epoch 00011: val_accuracy improved from 0.59188 to 0.65957, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 576ms/step - loss: 0.1737 - accuracy: 0.9449 - val_loss: 1.1963 - val_accuracy: 0.6596\n",
      "Epoch 12/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9492\n",
      "Epoch 00012: val_accuracy improved from 0.65957 to 0.75822, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 37s 571ms/step - loss: 0.1624 - accuracy: 0.9492 - val_loss: 0.7992 - val_accuracy: 0.7582\n",
      "Epoch 13/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9512\n",
      "Epoch 00013: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 37s 571ms/step - loss: 0.1480 - accuracy: 0.9512 - val_loss: 0.9324 - val_accuracy: 0.7273\n",
      "Epoch 14/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9613\n",
      "Epoch 00014: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 37s 572ms/step - loss: 0.1249 - accuracy: 0.9613 - val_loss: 1.1031 - val_accuracy: 0.7195\n",
      "Epoch 15/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9574\n",
      "Epoch 00015: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 38s 584ms/step - loss: 0.1356 - accuracy: 0.9574 - val_loss: 1.0403 - val_accuracy: 0.6983\n",
      "Epoch 16/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1076 - accuracy: 0.9647\n",
      "Epoch 00016: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 37s 569ms/step - loss: 0.1076 - accuracy: 0.9647 - val_loss: 2.1200 - val_accuracy: 0.6035\n",
      "Epoch 17/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9753\n",
      "Epoch 00017: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 37s 562ms/step - loss: 0.0815 - accuracy: 0.9753 - val_loss: 1.1387 - val_accuracy: 0.7021\n",
      "Epoch 18/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9666\n",
      "Epoch 00018: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 37s 570ms/step - loss: 0.1112 - accuracy: 0.9666 - val_loss: 1.3321 - val_accuracy: 0.6615\n",
      "Epoch 19/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0945 - accuracy: 0.9720\n",
      "Epoch 00019: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 37s 575ms/step - loss: 0.0945 - accuracy: 0.9720 - val_loss: 3.8873 - val_accuracy: 0.4139\n",
      "Epoch 20/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9700\n",
      "Epoch 00020: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 39s 593ms/step - loss: 0.0970 - accuracy: 0.9700 - val_loss: 2.0327 - val_accuracy: 0.5416\n",
      "Epoch 21/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9700\n",
      "Epoch 00021: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 40s 613ms/step - loss: 0.0875 - accuracy: 0.9700 - val_loss: 1.4468 - val_accuracy: 0.6750\n",
      "Epoch 22/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9744\n",
      "Epoch 00022: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 39s 594ms/step - loss: 0.0807 - accuracy: 0.9744 - val_loss: 1.2509 - val_accuracy: 0.7041\n",
      "Epoch 23/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9758\n",
      "Epoch 00023: val_accuracy did not improve from 0.75822\n",
      "65/65 [==============================] - 38s 583ms/step - loss: 0.0777 - accuracy: 0.9758 - val_loss: 1.3626 - val_accuracy: 0.6596\n",
      "Epoch 24/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9768\n",
      "Epoch 00024: val_accuracy improved from 0.75822 to 0.78143, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 38s 590ms/step - loss: 0.0720 - accuracy: 0.9768 - val_loss: 0.7669 - val_accuracy: 0.7814\n",
      "Epoch 25/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0729 - accuracy: 0.9782\n",
      "Epoch 00025: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 38s 581ms/step - loss: 0.0729 - accuracy: 0.9782 - val_loss: 0.8729 - val_accuracy: 0.7737\n",
      "Epoch 26/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9869\n",
      "Epoch 00026: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 38s 580ms/step - loss: 0.0564 - accuracy: 0.9869 - val_loss: 1.1549 - val_accuracy: 0.7311\n",
      "Epoch 27/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0937 - accuracy: 0.9710\n",
      "Epoch 00027: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 38s 577ms/step - loss: 0.0937 - accuracy: 0.9710 - val_loss: 8.7193 - val_accuracy: 0.2611\n",
      "Epoch 28/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0745 - accuracy: 0.9773\n",
      "Epoch 00028: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 38s 587ms/step - loss: 0.0745 - accuracy: 0.9773 - val_loss: 1.9033 - val_accuracy: 0.5938\n",
      "Epoch 29/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0844 - accuracy: 0.9739\n",
      "Epoch 00029: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 38s 590ms/step - loss: 0.0844 - accuracy: 0.9739 - val_loss: 1.9228 - val_accuracy: 0.6518\n",
      "Epoch 30/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9821\n",
      "Epoch 00030: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 38s 589ms/step - loss: 0.0684 - accuracy: 0.9821 - val_loss: 1.2466 - val_accuracy: 0.7215\n",
      "Epoch 31/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0708 - accuracy: 0.9836\n",
      "Epoch 00031: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 40s 608ms/step - loss: 0.0708 - accuracy: 0.9836 - val_loss: 1.1332 - val_accuracy: 0.7176\n",
      "Epoch 32/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0620 - accuracy: 0.9831\n",
      "Epoch 00032: val_accuracy did not improve from 0.78143\n",
      "65/65 [==============================] - 38s 590ms/step - loss: 0.0620 - accuracy: 0.9831 - val_loss: 1.0515 - val_accuracy: 0.7427\n",
      "Epoch 33/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9831\n",
      "Epoch 00033: val_accuracy improved from 0.78143 to 0.81818, saving model to saved_models\\model_5.h5\n",
      "65/65 [==============================] - 38s 588ms/step - loss: 0.0597 - accuracy: 0.9831 - val_loss: 0.6852 - val_accuracy: 0.8182\n",
      "Epoch 34/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9855\n",
      "Epoch 00034: val_accuracy did not improve from 0.81818\n",
      "65/65 [==============================] - 38s 591ms/step - loss: 0.0467 - accuracy: 0.9855 - val_loss: 1.7854 - val_accuracy: 0.6576\n",
      "Epoch 35/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9753\n",
      "Epoch 00035: val_accuracy did not improve from 0.81818\n",
      "65/65 [==============================] - 38s 586ms/step - loss: 0.0753 - accuracy: 0.9753 - val_loss: 1.1226 - val_accuracy: 0.7447\n",
      "Epoch 36/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9811\n",
      "Epoch 00036: val_accuracy did not improve from 0.81818\n",
      "65/65 [==============================] - 38s 590ms/step - loss: 0.0564 - accuracy: 0.9811 - val_loss: 0.7738 - val_accuracy: 0.7776\n",
      "Epoch 37/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9792\n",
      "Epoch 00037: val_accuracy did not improve from 0.81818\n",
      "65/65 [==============================] - 38s 587ms/step - loss: 0.0623 - accuracy: 0.9792 - val_loss: 1.1237 - val_accuracy: 0.7427\n",
      "Epoch 38/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9773\n",
      "Epoch 00038: val_accuracy did not improve from 0.81818\n",
      "65/65 [==============================] - 39s 604ms/step - loss: 0.0740 - accuracy: 0.9773 - val_loss: 1.6278 - val_accuracy: 0.6905\n",
      "Epoch 39/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9710\n",
      "Epoch 00039: val_accuracy did not improve from 0.81818\n",
      "65/65 [==============================] - 40s 608ms/step - loss: 0.0758 - accuracy: 0.9710 - val_loss: 1.1459 - val_accuracy: 0.7350\n",
      "Epoch 40/40\n",
      "65/65 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9811\n",
      "Epoch 00040: val_accuracy did not improve from 0.81818\n",
      "65/65 [==============================] - 39s 606ms/step - loss: 0.0630 - accuracy: 0.9811 - val_loss: 1.0102 - val_accuracy: 0.7756\n",
      "17/17 [==============================] - 2s 94ms/step - loss: 0.6852 - accuracy: 0.8182\n"
     ]
    }
   ],
   "source": [
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "\n",
    "save_dir = 'saved_models//'\n",
    "fold_var = 1\n",
    "\n",
    "for train_index, val_index in skf.split(np.zeros(2585),Y):\n",
    "\ttraining_data = train_data.iloc[train_index]\n",
    "\tvalidation_data = train_data.iloc[val_index]\n",
    "\n",
    "\ttrain_data_generator = idg.flow_from_dataframe(training_data, directory = '../Dataset/Train',\n",
    "\t\t\t\t\t\t       x_col = \"filename\", y_col = \"label\",\n",
    "\t\t\t\t\t\t       class_mode = \"categorical\", shuffle = True,color_mode=\"grayscale\",target_size=(224,224),batchsize=32)\n",
    "\tvalid_data_generator  = idg.flow_from_dataframe(validation_data, directory =  '../Dataset/Train',\n",
    "\t\t\t\t\t\t\tx_col = \"filename\", y_col = \"label\",\n",
    "\t\t\t\t\t\t\tclass_mode = \"categorical\", shuffle = True,color_mode=\"grayscale\",target_size=(224,224),batchsize=32)\n",
    "\n",
    "\t# CREATE NEW MODEL\n",
    "\tmodel = create_new_model()\n",
    "\t# COMPILE NEW MODEL\n",
    "\tmodel.compile(loss='categorical_crossentropy',\n",
    "\t\t      optimizer=\"Adam\",\n",
    "\t\t      metrics=['accuracy'])\n",
    "\n",
    "\t# CREATE CALLBACKS\n",
    "\tcheckpoint = tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var),\n",
    "\t\t\t\t\t\t\tmonitor='val_accuracy', verbose=1,\n",
    "\t\t\t\t\t\t\tsave_best_only=True, mode='max')\n",
    "\tcallbacks_list = [checkpoint]\n",
    "\t# There can be other callbacks, but just showing one because it involves the model name\n",
    "\t# This saves the best model\n",
    "\t# FIT THE MODEL\n",
    "\thistory = model.fit(train_data_generator,\n",
    "\t\t\t    epochs=40,\n",
    "\t\t\t    callbacks=callbacks_list,\n",
    "\t\t\t    validation_data=valid_data_generator)\n",
    "\t#PLOT HISTORY\n",
    "\t#\t\t:\n",
    "\t#\t\t:\n",
    "\n",
    "\t# LOAD BEST MODEL to evaluate the performance of the model\n",
    "\tmodel.load_weights(\"saved_models/model_\"+str(fold_var)+\".h5\")\n",
    "\n",
    "\tresults = model.evaluate(valid_data_generator)\n",
    "\tresults = dict(zip(model.metrics_names,results))\n",
    "\n",
    "\tVALIDATION_ACCURACY.append(results['accuracy'])\n",
    "\tVALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "\ttf.keras.backend.clear_session()\n",
    "\n",
    "\tfold_var += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy']) #validation\n",
    "plt.plot(history.history['val_accuracy']) #train\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}